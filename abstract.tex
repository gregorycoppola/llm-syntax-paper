\begin{abstract}
    This paper investigates the syntactic capabilities of large language models (LLMs) through a series of experiments evaluating ChatGPT’s performance on dependency parsing tasks. We begin by assessing the model in a zero-shot setting and find that while it performs poorly as a full parser, it achieves strong results on decomposed subtasks such as part-of-speech tagging. We then examine its ability to reason about syntactic ambiguity, using prepositional phrase (PP) attachment as a case study. Across 20 challenging examples, ChatGPT significantly outperforms a traditional Stanford dependency parser (95\% vs. 50\% accuracy), with statistical significance confirmed via binomial testing. Extending this analysis, we show that ChatGPT can not only detect parser errors, but also explain them in linguistically coherent terms, with human raters judging 95\% of its justifications as valid. Finally, we introduce an agentic architecture in which ChatGPT iteratively critiques and revises parses until convergence. Applied to 100 sentences, this looped system improves labeled attachment score by 8.6 percentage points over baseline parses, with 89\% of edits verified as correct. These findings suggest a shift in the role of LLMs from generators of structured output to evaluators and editors of syntactic structure—offering new directions for hybrid, human-in-the-loop NLP workflows.
\end{abstract}