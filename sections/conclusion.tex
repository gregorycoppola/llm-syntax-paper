\label{sec:conclusion}
We have shown through a mix of past evidence and new results that large language models {\em can} in fact act reliably as linguists, especially with regards to the kinds of syntactic labeling tasks that would be required to do open-domain semantic parsing.

We have specifically shown that for the specific difficult case of {\em prepositional phrase attachment}, that a ``zero shot'' LLM performance is comparable to that of a human, and far beyond that of the Stanford Stanza parser, which we take to be a representation of the state-of-the-art.

We also validated other aspects of the syntactic parse--POS tags and main verb identification (and possibly argument identification)--through manual evaluation to show that these tasks also work.

We believe that these constitute a ``prototype'' for the thesis that large language models {\em can} act as either fully-automated or largely automated linguists.

We call it a ``prototype'' because we believe that in order to make a ``full'' working parser, it would be required to figure out a comprehensive automatic (aka. ``agentic'') workflow to actually do {\em full} coverage parsing.

In other words, we believe that these results suggest that LLM's {\em do} have the ability to label syntactic information. Now, to prove the thesis that LLM's can do {\em enough} syntactic labeling to arrive at high-quality open-domain semantic parses, we need to figure out {\em which} workflow of LLM prompts and other function calls will arrive at high-quality parses, and demonstrate that one exists.