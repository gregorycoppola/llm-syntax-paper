% Literature review section content will go here 
Recent work has explored the ability of large language models (LLMs) to serve as syntactic annotators, generating linguistic labels such as part-of-speech (POS) tags, dependency trees, and constituency parses. These models can operate in zero-shot, few-shot, or fine-tuned settings, and have shown varying degrees of success depending on the syntactic task, language, and model scale.

\subsection{POS Tagging and Shallow Syntax}

\citet{blevins2023llmpos} examined GPT-3 and GPT-J models on POS tagging, NER, and chunking tasks. Few-shot prompting with GPT-J 6B reached 79\% accuracy for POS tagging, with GPT-3 (175B) scoring slightly lower. While this lags behind traditional supervised taggers (\textasciitilde96\%), it demonstrates the models' internalized syntactic knowledge.

\citet{lai2023chatgptpos} used zero-shot prompting with ChatGPT across 17 languages. Using prompts in either English or the target language, they achieved average POS accuracy of 84--85\%, outperforming fine-tuned XLM-R in most languages.

\citet{machado2024portpos} evaluated GPT-3, LLaMA-7B, and a Brazilian Portuguese LLM on POS tagging. With 10-shot prompts, GPT-3 achieved 90\% accuracy, significantly outperforming the smaller models.

\subsection{Agentic Prompting for Translation Tasks}

Building on a growing body of work that explores large language models (LLMs) as agentic linguistic annotators \citep[e.g.,][]{lai2023chatgptpos, blevins2023llmpos}, \citet{jiao2024gradable} investigate the extent to which LLMs can behave as adaptive agents in machine translation tasks. Rather than retraining or fine-tuning models, their approach leverages prompt engineering to steer ChatGPT's translation outputs across a continuum of linguistic complexity and context awareness.

They introduce the \textbf{T3S taxonomy}, which categorizes translation prompts into five levels of increasing detail and linguistic guidance. The dimensions of this taxonomy—expression type, translation style, part-of-speech (POS) information, and few-shot examples—mirror the types of scaffolding used in syntactic annotation tasks. At higher levels (e.g., Level 4), prompts ask ChatGPT to revise and proofread its own output, encouraging self-monitoring behavior akin to agentic reasoning.

Experimental results on the FLORES-101 Chinese--English corpus show a strong correlation between prompt complexity and translation quality, as measured by BLEU, ROUGE, and human evaluation. Notably, Level 4 prompts outperform even GPT-4's zero-shot baseline. These findings suggest that translation quality can be significantly improved through carefully structured, linguistically informed prompts—demonstrating the capacity of LLMs to act as agentic language processors even in high-level tasks like translation.

\subsection{Constituency Parsing}

\citet{bai2023llmconst} tested GPT-3.5, GPT-4, and LLaMA models on English constituency parsing. GPT-4 achieved an F1 of 73 with five-shot prompting, while open-weight LLaMA models scored below 30 F1.

\citet{tian2024chunkprompt} introduced a chunk-then-parse prompting approach using chain-of-thought (CoT) reasoning. This improved GPT-4's parsing quality on English and Chinese datasets, recovering deeper tree structures.

\subsection{The CoNLL Task and Format}

The CoNLL shared tasks have been central to advancing dependency parsing research, establishing standardized formats and evaluation frameworks. The original CoNLL-X shared task \citep{buchholz2006conll} introduced multilingual dependency treebanks in a standardized format, marking a major milestone in the field. This was followed by the CoNLL 2007 shared task \citep{nivre2007conll}, which expanded the language coverage and improved annotation consistency.

The modern Universal Dependencies (UD) project \citep{nivre2016universal} represents the current state of the art in dependency treebank development. UD provides a consistent annotation scheme across languages using the CoNLL-U format, which extends the original CoNLL format with additional features like morphological attributes and enhanced dependencies. This standardized format has become the de facto standard for evaluating dependency parsers, including recent work with LLMs.

\subsection{Dependency Parsing}

While constituency parsing has shown promising results with LLMs, dependency parsing remains a challenging task. Current approaches often require fine-tuning or specialized prompting strategies to achieve competitive performance. A notable exception is \citet{hromei2024udeppllama}'s U-DepPLLaMA, which reframes dependency parsing as a sequence-to-sequence task, where the model learns to generate bracketed representations of dependency trees directly from raw sentences. Despite the absence of task-specific architecture, U-DepPLLaMA achieves competitive performance across 50 Universal Dependencies treebanks in 26 languages, and shows robustness to sentence length and complexity. This work demonstrates that autoregressive LLMs can learn structured syntactic representations purely through fine-tuning.

\subsection{Zero-Shot Dependency Parsing}

Recent research has explored the potential for large language models (LLMs), such as ChatGPT, to perform dependency parsing in a zero-shot setting. While traditional approaches rely on task-specific architectures and supervised training, \citet{lin2023chatgpt} investigate whether ChatGPT can generate syntactic dependency trees when prompted directly in CoNLL format.

Their findings indicate that ChatGPT exhibits some emergent syntactic awareness, and can produce partially valid dependency parses for English and Chinese. However, its performance remains far below that of supervised parsers, with LAS scores of only 28.61 on PTB and 11.93 on CTB5. Furthermore, outputs often require extensive post-processing due to format inconsistencies, token omissions, or labeling errors. Other LLMs (e.g., Vicuna, HuggingChat) fail to generate valid output even when given one-shot examples.

Thus, while ChatGPT shows promise in demonstrating linguistic generalization, it does not function as a reliable one-shot dependency parser. Instead, it reflects an early-stage capability that may support downstream linguistic analysis rather than robust structured prediction.

\subsection{Summary of Methods and Trends}

\begin{table}[ht]
\centering
\begin{tabular}{p{3.3cm} p{2cm} p{2cm} p{2.5cm} p{3.5cm}}
\toprule
\textbf{Study} & \textbf{Model} & \textbf{Task} & \textbf{Method} & \textbf{Result / Remark} \\
\midrule
Blevins et al. (2023) & GPT-3, GPT-J & POS, Chunk & Few-shot & Up to 79\% POS (GPT-J); below SOTA \\
Lai et al. (2023) & ChatGPT & POS (UD) & Zero-shot & Avg. 85\% accuracy (17 langs) \\
Machado \& Ruiz (2024) & GPT-3 & POS (PT-BR) & Few-shot & 90\% POS accuracy \\
Jiao et al. (2024) & ChatGPT & Translation & T3S Taxonomy & Level 4 prompts beat GPT-4 baseline \\
Bai et al. (2023) & GPT-4, LLaMA & Constituency & Few-shot & GPT-4: 73 F1; LLaMA: $<$30 F1 \\
Tian et al. (2024) & GPT-4, GPT-3.5 & Constituency & Chunk + CoT & Improved tree depth and quality \\
Hromei et al. (2024) & U-DepPLLaMA & Dependency & Fine-tuned & Competitive on 50 UD treebanks \\
Lin et al. (2023) & ChatGPT & Dependency & Zero-shot & 28.61 LAS (PTB), 11.93 LAS (CTB5) \\
\bottomrule
\end{tabular}
\caption{LLMs for syntactic annotation: task, model, method, and performance summary.}
\label{tab:llm_syntax}
\end{table}

These studies demonstrate that LLMs can approximate human-level syntactic annotation, particularly for POS tagging. However, performance in parsing (especially full dependency trees) remains variable without fine-tuning. Prompt design, instruction tuning, and model scale all contribute to improved results, and multilingual evaluation remains a critical benchmark for future progress.


