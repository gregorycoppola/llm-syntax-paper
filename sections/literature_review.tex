% Literature review section content will go here
Recent work has explored the ability of large language models (LLMs) to serve as syntactic annotators, generating linguistic labels such as part-of-speech (POS) tags, dependency trees, and constituency parses. These models operate in zero-shot, few-shot, or fine-tuned settings and exhibit varying levels of success depending on the task, language, and model scale.

However, despite these advances, fundamental questions remain about the linguistic competence of LLMs. For instance, \citet{hu2020systematic} present a large-scale evaluation of syntactic generalization that reveals several striking limitations in current neural language models. Contrary to expectations, perplexity—a standard benchmark for language modeling—was poorly correlated with syntactic generalization ability: models with lower perplexity often performed worse on syntax-targeted tests, undermining the assumption that broad coverage leads to deeper linguistic competence. Standard LSTM architectures, even when trained on tens of millions of tokens, failed to robustly learn basic syntactic phenomena such as subject–verb agreement, and performed at near-chance levels on long-distance dependencies and center-embedding constructions. Transformer-based models like GPT-2 achieved stronger performance, but much of this success was attributed to preprocessing artifacts such as subword tokenization rather than architectural advantages. Perhaps most notably, even the best models consistently failed on syntactic licensing tests, such as those involving negative polarity items—phenomena that humans acquire early and handle reliably. These results suggest that many linguistic generalizations that are trivial for humans remain surprisingly elusive for even the most capable neural language models.

\subsection{POS Tagging and Shallow Syntax}

A key development in standardized syntactic annotation came with \citet{petrov2012universal}'s introduction of a universal part-of-speech tagset, mapping diverse language-specific tag inventories to a shared 12-tag schema to support multilingual and cross-lingual syntactic processing. This standardization has been crucial for evaluating POS tagging performance across languages and models.

\subsection{Dependency Parsing}

Moving from shallow to deep syntax, dependency parsing remains a more challenging task for LLMs. Traditional approaches rely heavily on supervised learning and task-specific architectures. However, \citet{hromei2024udeppllama}'s U-DepPLLaMA reframes dependency parsing as a sequence-to-sequence task, where the model learns to generate bracketed representations of dependency trees directly from raw sentences. Without relying on task-specific architecture, U-DepPLLaMA achieves competitive performance across 50 Universal Dependencies treebanks in 26 languages, demonstrating that autoregressive LLMs can internalize structured syntactic representations through fine-tuning.

The challenge of multilingual dependency parsing has been a longstanding focus of research. \citet{mcdonald2011multi} demonstrated that delexicalized dependency parsers could be effectively transferred across languages, showing that syntactic structure could be learned independently of lexical content. This insight has influenced modern approaches to cross-lingual parsing, including recent work with LLMs.

\paragraph{The CoNLL Format.} The CoNLL shared tasks have played a central role in advancing dependency parsing by establishing standardized formats and evaluation frameworks. The original CoNLL-X shared task \citep{buchholz2006conll} introduced multilingual dependency treebanks in a unified format. This was followed by CoNLL 2007 \citep{nivre2007conll}, which expanded language coverage and improved annotation consistency.

A pivotal development in representation came with \citet{de2006generating}'s Stanford Typed Dependencies, which provided a syntactically and semantically motivated dependency scheme. These efforts influenced the Universal Dependencies (UD) project \citep{nivre2016universal}, which today serves as the primary framework for evaluating dependency parsers in the CoNLL-U format.

\subsection{Zero-Shot Dependency Parsing}

Recent research has explored whether LLMs like ChatGPT can perform dependency parsing in a zero-shot setting. \citet{lin2023chatgpt} investigated whether ChatGPT could generate dependency trees when prompted directly in CoNLL format. While the model showed some emergent syntactic awareness and produced partially valid parses, performance remained far below that of supervised parsers, with LAS scores of only 28.61 on PTB and 11.93 on CTB5. Outputs also required significant post-processing due to formatting inconsistencies and labeling errors. Other LLMs (e.g., Vicuna, HuggingChat) failed to generate valid output even with one-shot examples.

Thus, while promising, zero-shot dependency parsing with ChatGPT is not yet reliable. However, the observed generalization suggests potential for supporting downstream linguistic analysis.

\subsection{Semantic Parsing and Compositional Semantics}

The relationship between syntactic and semantic parsing has long been a central concern in NLP. Early work by \citet{zelle1996} introduced one of the first data-driven semantic parsers, mapping natural language to Prolog queries via inductive logic programming. This line of research evolved through grammar-based and statistical approaches.

\citet{zettlemoyer2005} proposed a method for learning Combinatory Categorial Grammars (CCGs) from utterance-logical form pairs. This was extended by \citet{zettlemoyer2007online}, who introduced an online learning approach for more scalable parsing. \citet{bos2004} demonstrated wide-coverage semantic representations with CCGs, and \citet{kwiatkowski2010} showed how probabilistic grammars could be induced from logical forms.

An alternative view came from \citet{wong2006learning}, who framed semantic parsing as a machine translation problem using synchronous grammars. \citet{wong2007} incorporated lambda calculus to enhance compositional expressivity.

Later work moved toward weak supervision. \citet{clarke2010} learned from question-answer pairs instead of logical forms, introducing supervision via task outcomes. \citet{goldwasser2011confidence} and \citet{artzi2011} leveraged weak signals and bootstrapping from dialogue data. A major step came with \citet{liang2013learning}'s Dependency-Based Compositional Semantics (DCS), which induced semantic parsers from QA supervision using a dependency-style formalism.

This body of work underscores how structured syntax informs compositional semantics, laying the groundwork for evaluating how LLMs internalize both levels of linguistic structure.

Recent work has further explored LLMs' capabilities in semantic parsing. \citet{liu2023llm} investigate the zero-shot capabilities of GPT-3.5 and GPT-4 in semantic parsing tasks, including Text-to-SQL and AMR parsing. They find that while both models show strong performance in zero-shot scenarios, GPT-4 significantly outperforms GPT-3.5, particularly in complex parsing tasks. Their work highlights the importance of model scale and architecture in achieving robust semantic parsing capabilities.

\citet{sun-etal-2023-battle} conduct a comprehensive comparison of six prominent LLMs (Dolly, LLaMA, Vicuna, Guanaco, Bard, and ChatGPT) in Text-to-SQL parsing across nine benchmark datasets. Their systematic evaluation using five different prompting strategies reveals that while open-source models have made significant progress through instruction-tuning, they still lag behind closed-source models like GPT-3.5. This finding underscores the ongoing challenges in developing open-source alternatives that can match the performance of commercial LLMs in complex parsing tasks.

\citet{liu2023llm} also examine the impact of different prompting strategies on parsing performance, finding that structured prompts that explicitly guide the model through the parsing process tend to yield better results than simple task descriptions. They also investigate the role of in-context learning, showing that carefully selected examples can significantly improve parsing accuracy, particularly for complex queries.

\subsection{Historical Evolution and State of the Art in Text-to-SQL}

The task of translating natural language questions into executable SQL queries---commonly referred to as \emph{text-to-SQL}---has undergone significant transformation over the past decade. This section outlines the major milestones and the current landscape of text-to-SQL research, focusing on the interplay between evolving model architectures and data resources, and separating proprietary and open-source approaches.

\subsubsection{Early Systems and the Rise of Benchmarks}

Initial efforts in text-to-SQL were rule-based and constrained to narrow domains. The advent of neural networks enabled more flexible models. \citet{zhong2017seq2sql} introduced \texttt{Seq2SQL}, an early neural approach using reinforcement learning, marking the shift toward end-to-end learning.

The release of the \texttt{Spider} dataset by \citet{yu2018spider} was a critical turning point. Designed for complex, cross-domain queries, \texttt{Spider} quickly became the de facto benchmark for evaluating generalization in text-to-SQL systems. Its influence extended to shaping evaluation metrics and driving architectural innovations.

\subsubsection{Advances through Pretrained Language Models}

Following the success of general-purpose pre-trained language models (PLMs), researchers began leveraging them for semantic parsing. Techniques such as schema linking and representation learning, exemplified in \texttt{RESDSQL} by \citet{li2023resdsql}, pushed the performance of PLM-based systems on \texttt{Spider} and similar datasets.

Further, \citet{deng2021structure} demonstrated that grounding pretraining in schema and table structures could substantially improve performance, introducing structure-aware PLM training. These approaches, however, were limited by the fixed capacity of PLMs and required task-specific tuning.

\subsubsection{Transition to Large Language Models (LLMs)}

The recent emergence of Large Language Models (LLMs) has transformed the text-to-SQL landscape. Studies such as \citet{rajkumar2022evaluating} and \citet{pourreza2023dinsql} evaluated LLMs like GPT-3 and GPT-4 in zero- and few-shot regimes, showing strong performance without explicit training. \texttt{DIN-SQL} in particular introduced decomposition and self-correction to improve in-context learning (ICL).

Despite these advances, proprietary LLMs pose challenges for reproducibility and data privacy. To address this, the community has turned to open-source LLMs and training pipelines.

\subsubsection{Open-Source LLMs and Training Pipelines}

Recent work by \citet{li2024codes} proposed \texttt{CodeS}, a pre-training and fine-tuning framework specifically designed for SQL generation. It builds upon code-specific language models like StarCoder and incorporates SQL-augmented corpora for improved domain alignment.

Complementarily, \citet{hong2024knowledge} proposed \texttt{Knowledge-to-SQL}, an expert-augmented framework using fine-tuned models to inject domain knowledge and schema-aware reasoning, showing promising results on realistic and adversarial datasets.

\subsubsection{Benchmarking the Landscape}

A comprehensive benchmark comparison by \citet{gao2024benchmark} contrasted both proprietary and open-source LLMs across multiple datasets, including \texttt{Spider}, \texttt{BIRD}, and \texttt{Spider-Realistic}. Their findings illustrate a performance gap between closed and open models but highlight the rapid improvement of open-source alternatives through fine-tuning and prompt engineering.

\subsubsection{Current Challenges and Future Directions}

While LLMs show remarkable flexibility, the gap between controlled benchmarks and real-world deployment remains. Robustness to vague queries, schema complexity, and low-resource domains are active areas of research. Models like \texttt{SQLNet}~\cite{xu2017sqlnet} and newer decomposition-based methods suggest that combining symbolic reasoning with generative models may offer a path forward.

As the field continues to mature, there is a growing emphasis on interpretability, cost-efficiency, and privacy---challenges especially pertinent for industrial deployment. The interplay between open-source LLMs and domain-adapted fine-tuning pipelines marks a promising direction for creating practical, general-purpose natural language interfaces to databases.

\paragraph{Conclusion.}
Taken together, these studies highlight the expanding capacity of LLMs to perform syntactic and semantic analysis across languages and tasks. From POS tagging to deep compositional semantics, LLMs increasingly demonstrate internalized linguistic structure. Yet consistent challenges in structured prediction—especially for multilingual and zero-shot scenarios—indicate that traditional parsing insights remain essential. This review sets the stage for our own investigation into [insert your focus here], which further explores the interface between LLM prompting, structure induction, and linguistic generalization.

