% Literature review section content will go here
Recent work has explored the ability of large language models (LLMs) to serve as syntactic annotators, generating linguistic labels such as part-of-speech (POS) tags, dependency trees, and constituency parses. These models operate in zero-shot, few-shot, or fine-tuned settings and exhibit varying levels of success depending on the task, language, and model scale.

However, despite these advances, fundamental questions remain about the linguistic competence of LLMs. For instance, \citet{hu2020systematic} present a large-scale evaluation of syntactic generalization that reveals several striking limitations in current neural language models. Contrary to expectations, perplexity—a standard benchmark for language modeling—was poorly correlated with syntactic generalization ability: models with lower perplexity often performed worse on syntax-targeted tests, undermining the assumption that broad coverage leads to deeper linguistic competence. Standard LSTM architectures, even when trained on tens of millions of tokens, failed to robustly learn basic syntactic phenomena such as subject–verb agreement, and performed at near-chance levels on long-distance dependencies and center-embedding constructions. Transformer-based models like GPT-2 achieved stronger performance, but much of this success was attributed to preprocessing artifacts such as subword tokenization rather than architectural advantages. Perhaps most notably, even the best models consistently failed on syntactic licensing tests, such as those involving negative polarity items—phenomena that humans acquire early and handle reliably. These results suggest that many linguistic generalizations that are trivial for humans remain surprisingly elusive for even the most capable neural language models.

\subsection{Foundations of Generative Linguistics and Formal Semantics}

The development of generative linguistics in the second half of the 20th century marked a dramatic shift in the scientific study of language, emphasizing the formal structure of syntax and its relationship to meaning. This subsection highlights five foundational works that shaped this intellectual movement and laid the groundwork for modern syntactic and semantic theory.

\paragraph{Chomsky (1957): \textit{Syntactic Structures}.}
This work introduced the concept of a \emph{generative grammar}—a formal system capable of generating the infinite set of grammatical sentences in a language. Chomsky proposed a set of phrase structure rules and transformational rules, arguing that syntactic theory should be precise, explicit, and formally rigorous. The book also introduced the notion of deep structure and raised fundamental questions about the inadequacy of finite-state models for capturing human syntax. It effectively launched the generative paradigm and established syntax as a central object of linguistic inquiry.

\paragraph{Chomsky (1965): \textit{Aspects of the Theory of Syntax}.}
In this follow-up, Chomsky significantly extended his theoretical framework by distinguishing between \emph{competence} (the idealized knowledge of language) and \emph{performance} (actual language use). He refined the concept of deep vs. surface structure and introduced the T-model architecture of grammar. The work also proposed the theory of \emph{universal grammar}—a set of innate principles and parameters shared by all human languages—which became a cornerstone of subsequent research in generative syntax and language acquisition.

\paragraph{Katz and Fodor (1963): \textit{The Structure of a Semantic Theory}.}
This influential paper represented one of the earliest attempts to systematically integrate semantics into the generative framework. Katz and Fodor proposed a decompositional view of lexical meaning, where word senses are defined in terms of bundles of semantic features. Their model attempted to link syntactic structure to semantic interpretation, setting the stage for formal approaches to the syntax–semantics interface. Though later surpassed by model-theoretic semantics, their work was instrumental in establishing semantics as a formal, computationally tractable component of linguistic theory.

\paragraph{Jackendoff (1972): \textit{Semantic Interpretation in Generative Grammar}.}
Jackendoff extended the generative program by proposing a more elaborate mapping from syntactic structure to semantic interpretation. He introduced the idea of multiple levels of representation—syntactic, semantic, and phonological—and emphasized their interaction. His framework helped clarify the architecture of grammar and provided early formulations of what would become \emph{conceptual semantics}, influencing both generative and cognitive approaches to meaning.

\paragraph{Montague (1970): \textit{English as a Formal Language}.}
Montague revolutionized semantic theory by showing that natural languages could be treated with the same formal rigor as logical systems. He applied tools from model theory, lambda calculus, and type theory to build compositional semantics for fragments of English. This work laid the foundation for \emph{Montague Grammar}, a highly influential framework that unified syntax and semantics. Although developed independently of the Chomskyan tradition, Montague's work profoundly influenced generative semantics and was later integrated into transformational grammar by scholars such as Partee.

Together, these works represent the intellectual bedrock of modern linguistic theory. They established the principles of syntactic generation, formal semantic interpretation, and the architecture of linguistic competence, shaping decades of subsequent research in theoretical linguistics, psycholinguistics, and computational models of language.

\subsection{The Historical NLP Pipeline}

The development of natural language processing (NLP) has been marked by a series of methodological revolutions, each building on and extending the capabilities of its predecessors. This section traces the evolution of key components in the NLP pipeline, from early rule-based systems through statistical methods to modern neural approaches.

\textbf{Part-of-Speech Tagging.} The task of part-of-speech (POS) tagging—assigning grammatical categories like noun, verb, or adjective to words in a sentence—has evolved through several major paradigms over the past decades. Early work in the 1960s and 70s relied on rule-based approaches and manually curated lexicons. Greene and Kuno \cite{greene1963automatic} presented one of the earliest automatic tagging systems, combining dictionary lookups with hand-written disambiguation rules. The release of the Brown Corpus \cite{francis1979brown} marked a turning point by providing a large, annotated dataset for statistical analysis.

In the early 1990s, the statistical revolution in NLP led to probabilistic models becoming dominant. Kupiec \cite{kupiec1992robust} applied Hidden Markov Models (HMMs) to POS tagging, modeling the tagging task as a sequence labeling problem. Brill \cite{brill1995transformation} introduced a transformation-based learning approach that combined rule induction with error-driven updates, achieving competitive results with interpretable rules.

Machine learning continued to drive innovation. Ratnaparkhi \cite{ratnaparkhi1996maximum} introduced a Maximum Entropy tagger that incorporated contextual features in a flexible framework. Conditional Random Fields (CRFs), as formalized by Lafferty et al. \cite{lafferty2001conditional}, improved sequence labeling by modeling entire sequences of tags jointly, overcoming some of the limitations of HMMs.

The 2000s also saw the development of more feature-rich models. Toutanova et al. \cite{toutanova2003feature} proposed a cyclic dependency network that leveraged linguistic features like prefixes and suffixes, improving accuracy on benchmark corpora.

With the rise of deep learning in the 2010s, POS tagging transitioned into the neural era. Huang et al. \cite{huang2015bidirectional} demonstrated that bidirectional LSTM-CRF models could outperform traditional methods without the need for hand-engineered features. Akbik et al. \cite{akbik2018contextual} introduced contextual string embeddings, capturing rich character-level and word-level representations. The introduction of large pretrained language models like BERT \cite{devlin2019bert} further advanced the state of the art in POS tagging, especially when fine-tuned for sequence labeling tasks.

Finally, the push toward multilingual and cross-lingual tagging led to the creation of Universal Dependencies (UD) \cite{nivre2016universal}, which provided standardized POS tags and treebanks across dozens of languages, enabling consistent evaluation and transfer learning techniques.

\textbf{Phrase Structure Parsing.} Phrase structure parsing, also known as constituency parsing, is a foundational task in natural language processing (NLP) that aims to recover the hierarchical phrase-based structure of a sentence. Over the decades, the field has transitioned from symbolic rule-based systems to statistical models and, more recently, to neural and transformer-based architectures.

The theoretical basis for phrase structure parsing originated in the field of linguistics, with Chomsky's \textit{Syntactic Structures} \cite{chomsky1957syntactic} introducing the concept of generative grammars and context-free rules. These ideas laid the groundwork for early parsers, which relied on hand-crafted rules but struggled with ambiguity and scale.

The introduction of the Penn Treebank \cite{marcus1993building} marked a pivotal moment for empirical NLP. This large annotated corpus enabled the training and evaluation of statistical models, moving the field beyond rule-based systems. Leveraging this resource, Collins \cite{collins1997three} developed one of the first successful lexicalized probabilistic parsers, incorporating headwords into probabilistic context-free grammars (PCFGs) to improve disambiguation. In parallel, Charniak \cite{charniak1997statistical} proposed a PCFG-based parser that further demonstrated the viability of statistical parsing using rich lexical statistics.

As parsing performance plateaued under purely generative models, researchers shifted toward discriminative approaches. Collins and Koo \cite{collins2005discriminative} introduced reranking techniques, using discriminative models to select the best parse from a list of candidates generated by a baseline parser. Around the same time, McClosky et al. \cite{mcclosky2006effective} showed how semi-supervised learning via self-training could boost parsing performance by leveraging large amounts of unlabeled text, a strategy that prefigured later trends in unsupervised and transfer learning.

The next paradigm shift came with the rise of neural networks. Socher et al. \cite{socher2013parsing} pioneered the use of recursive neural networks for parsing, modeling the compositional structure of phrases directly in a vector space. This work demonstrated that deep learning could encode syntactic structure in a meaningful and learnable way. Soon after, Vinyals et al. \cite{vinyals2015grammar} reframed parsing as a sequence-to-sequence problem, using LSTMs to generate trees directly from input sentences—an early sign of the growing convergence between parsing and general-purpose sequence modeling.

The introduction of self-attention mechanisms led to a leap in performance and efficiency. Kitaev and Klein \cite{kitaev2018constituency} proposed a parser based on a self-attentive encoder that achieved state-of-the-art results without recurrent networks. They later extended their model with multilingual pretraining \cite{kitaev2019multilingual}, showing that transformer-based architectures like BERT could be effectively adapted for constituency parsing across languages.

Finally, Mrini et al. \cite{mrini2020rethinking} revisited the design of transformer architectures with an eye toward interpretability and efficiency, refining attention mechanisms specifically for parsing tasks. Their work represents a broader trend in NLP: optimizing pretrained models not just for raw performance, but also for transparency and adaptability.

\textbf{Dependency Parsing.} Dependency parsing aims to identify syntactic relationships between words in a sentence, typically in the form of directed, labeled graphs where each word depends on a syntactic head. Compared to phrase structure parsing, dependency representations are often more suitable for morphologically rich and free word order languages, making them a popular choice in multilingual and applied NLP tasks.

The theoretical foundation of dependency parsing can be traced back to Mel'čuk's seminal work on dependency syntax \cite{melcuk1988dependency}, which formalized the representation of syntactic relations in a way that was both linguistically expressive and computationally viable. Although primarily theoretical, Mel'čuk's framework laid the groundwork for many subsequent parsing systems.

A practical breakthrough came with Yamada and Matsumoto's data-driven transition-based parser \cite{yamada2003statistical}, which modeled parsing as a sequence of decisions made by a classifier—specifically a support vector machine. Their approach pioneered the use of machine learning in dependency parsing and inspired a generation of transition-based parsers.

In contrast, McDonald et al. introduced a graph-based alternative to dependency parsing \cite{mcdonald2005online}, treating the task as finding the maximum spanning tree over possible head-dependent arcs. This approach, trained with a structured perceptron, offered global optimization and quickly became a strong baseline. McDonald and Pereira \cite{mcdonald2006online} extended this work by incorporating higher-order features such as sibling and grandparent relationships, thereby improving parsing accuracy while maintaining tractability.

Concurrently, Nivre and colleagues released MaltParser \cite{nivre2006maltparser}, a flexible and widely adopted transition-based parsing toolkit that supports various parsing strategies and languages. Its configurability and speed made it a go-to solution for both academic research and practical applications.

Pushing the boundaries of expressiveness, Koo and Collins \cite{koo2010efficient} introduced an efficient third-order parser that incorporated rich structural features such as tri-siblings and grand-siblings into the graph-based framework. Their work demonstrated how higher-order relationships could improve accuracy without prohibitive computational costs.

Bridging the gap between transition- and graph-based methods, Zhang and Nivre \cite{zhang2011transition} showed that using beam search and non-local features in transition-based parsing could dramatically improve performance, allowing fast parsers to model longer-range dependencies more effectively.

The era of neural parsing began with the work of Chen and Manning \cite{chen2014fast}, who replaced hand-engineered feature templates with dense, learned representations using feedforward neural networks. This marked the first successful neural dependency parser and set the stage for deep learning approaches in syntactic analysis.

Dozat and Manning \cite{dozat2017deep} further advanced the state of the art with their biaffine parser, which combined BiLSTM encoders and attention-based scoring in a graph-based architecture. This model achieved exceptional accuracy and became the de facto baseline for modern dependency parsing.

Finally, Kondratyuk and Straka \cite{kondratyuk2019udpipe} leveraged multilingual BERT representations to build a single model capable of parsing 75 languages. This work illustrated the power of transfer learning and multilingual pretraining, solidifying dependency parsing's place in the transformer era.

\textbf{Semantic Parsing.} The relationship between syntactic and semantic parsing has long been a central concern in NLP. Early work by \citet{zelle1996} introduced one of the first data-driven semantic parsers, mapping natural language to Prolog queries via inductive logic programming. This line of research evolved through grammar-based and statistical approaches.

\citet{zettlemoyer2005} proposed a method for learning Combinatory Categorial Grammars (CCGs) from utterance-logical form pairs. This was extended by \citet{zettlemoyer2007online}, who introduced an online learning approach for more scalable parsing. \citet{bos2004} demonstrated wide-coverage semantic representations with CCGs, and \citet{kwiatkowski2010} showed how probabilistic grammars could be induced from logical forms.

An alternative view came from \citet{wong2006learning}, who framed semantic parsing as a machine translation problem using synchronous grammars. \citet{wong2007} incorporated lambda calculus to enhance compositional expressivity.

Later work moved toward weak supervision. \citet{clarke2010} learned from question-answer pairs instead of logical forms, introducing supervision via task outcomes. \citet{goldwasser2011confidence} and \citet{artzi2011} leveraged weak signals and bootstrapping from dialogue data. A major step came with \citet{liang2013learning}'s Dependency-Based Compositional Semantics (DCS), which induced semantic parsers from QA supervision using a dependency-style formalism.

\textbf{Text-to-SQL.} The task of translating natural language questions into executable SQL queries---commonly referred to as \emph{text-to-SQL}---has undergone significant transformation over the past decade. Initial efforts in text-to-SQL were rule-based and constrained to narrow domains. The advent of neural networks enabled more flexible models. \citet{zhong2017seq2sql} introduced \texttt{Seq2SQL}, an early neural approach using reinforcement learning, marking the shift toward end-to-end learning.

The release of the \texttt{Spider} dataset by \citet{yu2018spider} was a critical turning point. Designed for complex, cross-domain queries, \texttt{Spider} quickly became the de facto benchmark for evaluating generalization in text-to-SQL systems. Its influence extended to shaping evaluation metrics and driving architectural innovations.

Following the success of general-purpose pre-trained language models (PLMs), researchers began leveraging them for semantic parsing. Techniques such as schema linking and representation learning, exemplified in \texttt{RESDSQL} by \citet{li2023resdsql}, pushed the performance of PLM-based systems on \texttt{Spider} and similar datasets.

Further, \citet{deng2021structure} demonstrated that grounding pretraining in schema and table structures could substantially improve performance, introducing structure-aware PLM training. These approaches, however, were limited by the fixed capacity of PLMs and required task-specific tuning.

The recent emergence of Large Language Models (LLMs) has transformed the text-to-SQL landscape. Studies such as \citet{rajkumar2022evaluating} and \citet{pourreza2023dinsql} evaluated LLMs like GPT-3 and GPT-4 in zero- and few-shot regimes, showing strong performance without explicit training. \texttt{DIN-SQL} in particular introduced decomposition and self-correction to improve in-context learning (ICL).

Despite these advances, proprietary LLMs pose challenges for reproducibility and data privacy. To address this, the community has turned to open-source LLMs and training pipelines. Recent work by \citet{li2024codes} proposed \texttt{CodeS}, a pre-training and fine-tuning framework specifically designed for SQL generation. It builds upon code-specific language models like StarCoder and incorporates SQL-augmented corpora for improved domain alignment.

Complementarily, \citet{hong2024knowledge} proposed \texttt{Knowledge-to-SQL}, an expert-augmented framework using fine-tuned models to inject domain knowledge and schema-aware reasoning, showing promising results on realistic and adversarial datasets.

A comprehensive benchmark comparison by \citet{gao2024benchmark} contrasted both proprietary and open-source LLMs across multiple datasets, including \texttt{Spider}, \texttt{BIRD}, and \texttt{Spider-Realistic}. Their findings illustrate a performance gap between closed and open models but highlight the rapid improvement of open-source alternatives through fine-tuning and prompt engineering.

\subsection{LLMs and the Historical NLP Pipeline}

Recent work has demonstrated that large language models (LLMs) benefit not only from larger model size and better pretraining objectives, but also from the integration of external structure—be it in the form of symbolic knowledge, task-specific prompts, or compositional representations. This section surveys several prominent methods that extend the RASP paradigm through retrieval, structural supervision, or neuro-symbolic grounding to enhance LLM-based semantic parsing and generalization.

\paragraph{Syntactic Competence in LLMs.} While LLMs have shown promise in zero-shot and few-shot syntactic and semantic parsing, their ability to disambiguate lexical meaning and generalize to unseen concepts remains limited—especially in structured parsing tasks requiring precise, graph-based outputs. A recent line of work explores whether external knowledge sources can enhance LLM performance by augmenting their inputs with structured lexical information.

One of the most promising developments in this area is the \textbf{RASP} framework (\textit{Retrieval-Augmented Semantic Parsing}) proposed by \citet{zhang2024rasp}. RASP addresses a core challenge in neural semantic parsing: the tendency of LLMs to rely on heuristics, such as always predicting the most frequent sense of a word when encountering ambiguous or out-of-distribution concepts. This behavior often leads to ``lucky guesses'' and degrades the model's ability to robustly disambiguate word meanings in open-domain settings. RASP tackles this problem by injecting \textit{retrieved lexical glosses}—specifically from WordNet—into the model input, thereby grounding the model's output in explicit semantic context.

The method operates in two stages: a retrieval step and a generation step. During retrieval, all plausible WordNet synsets (senses) for each content word in the input are gathered, along with their glosses. These glosses are concatenated and prepended to the input text in a structured format. Then, a decoder-only LLM (e.g., Mistral, Gemma, Qwen, LLaMA) is prompted with this enriched input to generate Discourse Representation Structures (DRSs) that align with the correct word senses. Formally, this integrates two probabilities: \( p(o' \mid x) \) for retrieving relevant glosses, and \( p_{\text{decoder-only}}(o \mid x, o') \) for generating the output conditioned on both the original input and the retrieved information.

\paragraph{Semantic Parsing and Structure-Aware Augmentation.} Recent work has demonstrated that LLMs can be significantly enhanced when paired with structured lexical knowledge in context. Rather than relying solely on scale or additional training data, approaches like RASP leverage existing resources like WordNet to bridge the gap between LLM fluency and semantic precision. This approach is especially valuable in domains where generalization to rare or out-of-vocabulary concepts is critical.

\citet{cheng2023uprise} propose \textbf{UPRISE}, a retrieval-augmented framework that improves zero-shot performance in structured prediction by retrieving task-specific prompts. Rather than relying solely on handcrafted prompt templates, UPRISE builds a bank of example prompts and retrieves the most relevant ones based on the test input using dense similarity. These retrieved prompts are appended to the model input before decoding, effectively grounding generation in in-context prior examples.

\citet{liu2021kgbart} introduce \textbf{KG-BART}, a framework that incorporates subgraphs from external knowledge graphs—specifically ConceptNet—into the BART model to enhance commonsense reasoning. The model encodes these graphs alongside the input sequence, allowing the decoder to attend to both textual and symbolic representations. Although developed for commonsense QA, the approach demonstrates the value of integrating structured external knowledge into generative models.

\citet{lu2023structgpt} present \textbf{StructGPT}, a method that treats structured prediction as a sequence of programmatic actions using a domain-specific language (DSL). Rather than producing raw text, the LLM is prompted to generate parse trees or structured outputs by selecting from tool-like functions (e.g., "AddNode", "SetLabel"). This framing allows the LLM to reason in a constrained, compositional space, dramatically improving structure compliance.

\paragraph{Retrieval and Structure-Aware Augmentation.} \textbf{ATLAS} \citep{izacard2024atlas} is a general-purpose retrieval-augmented generation framework that fine-tunes LLMs to perform tasks conditioned on external retrieved text segments. Unlike systems that treat retrieval as static, ATLAS jointly learns both the retriever and generator end-to-end. Although originally proposed for QA and open-domain tasks, ATLAS has since been adapted to AMR parsing and other meaning representation tasks.

\citet{li2023structure} propose \textbf{structure-aware pretraining} (SAP), which modifies the pretraining objective of LLMs to emphasize structural relationships in data such as tables and parse trees. By pretraining on structured data with hierarchical and compositional constraints, the resulting models generalize better to downstream tasks like dense retrieval and Text-to-SQL.

\paragraph{Evaluation Challenges and Benchmarks.} \citet{bharadwaj2023benchclamp} introduce \textsc{BenchCLAMP}, a benchmark designed to evaluate pretrained language models on structured prediction tasks across nine datasets spanning seven output formalisms, including semantic and syntactic parsing. The authors propose a unified framework that supports both fine-tuning and few-shot prompting while enabling efficient constrained decoding via task-specific context-free grammars.

Recent work by \cite{ettinger2023expert} investigates whether large language models (LLMs) such as GPT-3, ChatGPT, and GPT-4 can serve as expert linguistic annotators in the task of Abstract Meaning Representation (AMR) parsing. While the models are able to reproduce the basic surface format of AMR graphs—demonstrating familiarity with its syntactic structure—they consistently fail to generate semantically accurate parses.

\paragraph{Conclusion.} Taken together, these studies highlight the expanding capacity of LLMs to perform syntactic and semantic analysis across languages and tasks. From POS tagging to deep compositional semantics, LLMs increasingly demonstrate internalized linguistic structure. Yet consistent challenges in structured prediction—especially for multilingual and zero-shot scenarios—indicate that traditional parsing insights remain essential. This review sets the stage for our own investigation into [insert your focus here], which further explores the interface between LLM prompting, structure induction, and linguistic generalization.

\subsection{LLM's as Linguists}
Recent experiments demonstrate that large language models (LLMs) can support the enrichment of lexical resources such as FrameNet by identifying new lexical units and generating annotated example sentences. Using zero-shot prompting, models like GPT-4o and Claude 3.5 Sonnet achieved high precision in classifying verbs into appropriate semantic frames and produced natural-sounding, grammatically correct example sentences annotated with core frame elements. Moreover, LLMs showed some ability to distinguish between aspectual verb classes (e.g., activities vs. achievements) based on syntactic diagnostics. However, the results also reveal important limitations. The quality of output depends heavily on prompt design, and models sometimes misinterpret verb senses or generate contextually ambiguous examples. Additionally, linguistic diagnostics (such as co-occurrence with temporal adverbials) do not always function reliably when applied to LLMs, due to the models' tendency to produce acceptable-sounding but semantically inconsistent constructions. These findings suggest that while LLMs are not yet capable of fully replacing expert annotation, they can serve as effective collaborators or second annotators in the process of developing semantic resources \citep{koeva2024}.

Recent work demonstrates that large language models (LLMs), such as GPT-3 (text-davinci-002), can accurately simulate human acceptability judgments of verb argument structure constructions, outperforming previous computational and theoretical models \citep{ambridge2024}. Unlike earlier approaches that required extensive hand-coded input, including pre-specified semantic features and manually curated training data, LLMs produce high correlations with human ratings with no such scaffolding. For example, GPT-3’s ratings of English causative constructions showed strong alignment with adult human judgments (e.g., $r = 0.92$, $\tau = 0.80$). Furthermore, the model generalized appropriately to novel verbs, indicating sensitivity to semantic properties. The authors argue that LLMs should be considered executable theories of language acquisition, with their architectures and training regimes encoding empirically testable hypotheses, in contrast to traditional models which rely on oversimplified, intuitively appealing abstractions.

Recent work demonstrates that large language models (LLMs) are capable of sophisticated metalinguistic reasoning across a range of linguistic tasks. Among the evaluated models, OpenAI's \texttt{o1-preview} stands out for its exceptional performance, particularly in tree-based syntactic analysis and abstract phonological rule induction. On tasks involving recursive syntactic structures and syntactic movement, \texttt{o1-preview} significantly outperforms GPT-3.5, GPT-4, and LLaMA 3.1, achieving near-ceiling accuracy in both syntactic tree construction and the identification of movement traces. Crucially, \texttt{o1-preview} also demonstrates the ability to generalize phonological rules from invented datasets, including cases involving unnatural or rare sound patterns. These findings suggest that with sufficient architectural enhancements, LLMs can acquire genuinely abstract grammatical knowledge, not merely memorize surface-level patterns \citep{begus2025}.

\paragraph{} \citet{blevins2023prompting} explore how large language models can be prompted to output structured linguistic analyses, including syntactic trees and morphological segmentation. The study finds that while models like GPT-3 perform well on surface-level morphological tasks, their ability to generate deep hierarchical structure is more limited and depends heavily on prompt phrasing. This paper is one of the first to systematically evaluate the effectiveness of prompting for eliciting explicit linguistic structures, showing both the promise and limits of treating LLMs as linguists via prompt engineering.

\paragraph{} \citet{behzad2023elqa} introduce ELQA, a curated corpus of metalinguistic questions and answers about English designed to evaluate the metalinguistic awareness of LLMs. The questions range from terminology and definitions to error identification and grammaticality judgments. Their analysis shows that while LLMs like GPT-3.5 can answer basic metalinguistic questions well, performance on more technical or abstract queries is mixed. The resource provides a benchmark for treating LLMs as language analysts rather than just generators.

\paragraph{} \citet{wilcox2018filler} examine whether RNN-based language models can learn and generalize filler--gap dependencies, a hallmark of syntactic competence in humans. Using psycholinguistic-inspired experiments, they show that LMs trained purely on text can exhibit sensitivity to unbounded dependencies such as wh-movement, suggesting an implicit understanding of hierarchical structure. However, their generalization is incomplete, and errors increase with more deeply embedded clauses. This work laid early groundwork for probing syntactic generalizations in LMs.

\paragraph{} \citet{gulordava2018colorless} demonstrate that LSTM-based language models trained on multilingual corpora can capture long-distance syntactic dependencies even in nonsensical ``colorless green'' sentences. The authors show that models trained without semantic grounding can still predict syntactically well-formed continuations, supporting the idea that language models develop abstract grammatical representations. This study is widely cited for arguing that hierarchical syntax can emerge from distributional learning alone.

\paragraph{} \citet{matusevych2022trees} test whether RNNs can learn the recursive structure of noun phrases using an artificial grammar learning paradigm. The study shows that RNNs trained on synthetic languages with hierarchical NP structure can generalize to novel recursive inputs. Importantly, this work uses techniques modeled after human learning experiments, providing a strong analogy between human learners and neural networks. It supports the claim that LMs can act like linguists under the right training and testing conditions.

\paragraph{} \citet{yedetore2023stimulus} ask whether LLMs trained on child-directed speech (a limited and more naturalistic corpus) can generalize hierarchical rules. Using Transformer models, they find that such networks struggle with hierarchical generalization, especially when data is sparse. This contributes to the ``poverty of the stimulus'' debate and suggests that LLMs may require more data than humans to achieve similar generalizations---unless architectural enhancements (like chain-of-thought) are introduced. The paper is especially relevant for evaluating the limits of LLMs as cognitive models of linguists.

\paragraph{Alivanistos et al. (2022)}
\citet{alivanistos2022prompting} present a framework in which prompting is used not just as a task input method, but as a probing mechanism to extract structured knowledge from pretrained language models. Specifically, they show how carefully designed prompts can be used to elicit entity and relation triples, effectively reconstructing knowledge graphs. The model's ability to complete these relations without additional fine-tuning demonstrates that LLMs retain a rich latent knowledge base. This aligns with the notion of treating LLMs as linguists, capable of metalinguistic analysis and structure inference when prompted appropriately.

\paragraph{Li et al. (2022)}
\citet{li2022probing} propose replacing traditional diagnostic probes (small classifiers trained on frozen model representations) with learned prompts to assess what language models know. Their approach, called "probing via prompting," involves training discrete or continuous prompts that elicit a model's latent knowledge in a zero-shot or few-shot setting. They show that prompting can serve the same purpose as post-hoc probes, but with the benefit of being closer to the model's natural inference behavior. This methodology strengthens the view that LLMs can be analyzed in the same way we might evaluate a linguistic informant.

\paragraph{Min et al. (2022)}
\citet{min2022rethinking} explore what aspects of in-context learning are responsible for LLM performance. Through a series of controlled experiments, they find that much of the performance can be attributed not to learning from the examples per se, but from strong prior knowledge in the model, especially about label distributions and common class patterns. Even when demonstrations are perturbed or semantically meaningless, models often perform well. This suggests that LLMs behave less like tabula rasa learners and more like agents with pre-existing linguistic knowledge---which can be elicited through structured prompting.

\paragraph{Press et al. (2022)}
\citet{press2022measuring} tackle the "compositionality gap" in LLMs, which is the mismatch between compositional generalization and model performance. They propose a method called "chain-of-thought with intermediate prompting," where tasks are decomposed into simpler subtasks and solved in sequence, with each output fed as context to the next. Their results show that LLMs can learn more compositional functions when intermediate reasoning steps are prompted explicitly. This approach provides a procedural lens on LLMs' linguistic competence, treating them as capable of assembling complex outputs from smaller linguistic primitives.

\paragraph{Zhou et al. (2022)}
\citet{zhou2022least} introduce the method of "least-to-most prompting," a strategy where LLMs are prompted to solve reasoning problems by building from simple to complex sub-problems. This mimics the pedagogical techniques of human learning and shows improved performance on tasks like multi-hop reasoning, arithmetic, and syntax-sensitive parsing. The method emphasizes the value of step-wise syntactic decomposition, and shows that LLMs can learn and apply rule-based structure when guided incrementally---much like a linguist following a deductive analysis.

\subsection{Categorial Grammar}

The origins of categorial grammar can be traced back to the early work of Ajdukiewicz, who proposed a formal system for representing syntactic structure using category assignments and functional composition \cite{ajdukiewicz1935syntaktische}. His pioneering ideas introduced the notion that syntactic categories could be combined through well-defined operations, laying the groundwork for later developments in mathematical linguistics.

Building on this, Bar-Hillel extended Ajdukiewicz's notation and explored its application to natural language syntax, introducing a quasi-arithmetical formalism for syntactic description \cite{barhillel1953quasi}. His work played a crucial role in bridging the gap between philosophical logic and formal grammar, and it anticipated several principles of modern computational linguistics.

A major formalization of categorial grammar was provided by Lambek, who introduced a type-logical approach that framed sentence structure within a logical calculus \cite{lambek1958mathematics}. The Lambek calculus remains a cornerstone of categorial grammar theory, offering a powerful and elegant method for capturing syntactic inference through type composition.

In the modern era, Steedman significantly advanced the theory with his work on Combinatory Categorial Grammar (CCG), a highly lexicalized and flexible grammar formalism \cite{steedman1996surface}. Steedman's contributions have been central to the application of categorial principles in computational linguistics and have influenced a range of systems in syntax and semantics.

Together, these works define the intellectual trajectory of categorial grammar, from its philosophical roots to its current role in formal and computational linguistics.
