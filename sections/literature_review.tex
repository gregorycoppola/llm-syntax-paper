% Literature review section content will go here
Recent work has explored the ability of large language models (LLMs) to serve as syntactic annotators, generating linguistic labels such as part-of-speech (POS) tags, dependency trees, and constituency parses. These models operate in zero-shot, few-shot, or fine-tuned settings and exhibit varying levels of success depending on the task, language, and model scale.

However, despite these advances, fundamental questions remain about the linguistic competence of LLMs. For instance, \citet{hu2020systematic} present a large-scale evaluation of syntactic generalization that reveals several striking limitations in current neural language models. Contrary to expectations, perplexity—a standard benchmark for language modeling—was poorly correlated with syntactic generalization ability: models with lower perplexity often performed worse on syntax-targeted tests, undermining the assumption that broad coverage leads to deeper linguistic competence. Standard LSTM architectures, even when trained on tens of millions of tokens, failed to robustly learn basic syntactic phenomena such as subject–verb agreement, and performed at near-chance levels on long-distance dependencies and center-embedding constructions. Transformer-based models like GPT-2 achieved stronger performance, but much of this success was attributed to preprocessing artifacts such as subword tokenization rather than architectural advantages. Perhaps most notably, even the best models consistently failed on syntactic licensing tests, such as those involving negative polarity items—phenomena that humans acquire early and handle reliably. These results suggest that many linguistic generalizations that are trivial for humans remain surprisingly elusive for even the most capable neural language models.

\subsection{History of Part-of-Speech Tagging}

The task of part-of-speech (POS) tagging—assigning grammatical categories like noun, verb, or adjective to words in a sentence—has evolved through several major paradigms over the past decades.

Early work in the 1960s and 70s relied on rule-based approaches and manually curated lexicons. Greene and Kuno \cite{greene1963automatic} presented one of the earliest automatic tagging systems, combining dictionary lookups with hand-written disambiguation rules. The release of the Brown Corpus \cite{francis1979brown} marked a turning point by providing a large, annotated dataset for statistical analysis.

In the early 1990s, the statistical revolution in NLP led to probabilistic models becoming dominant. Kupiec \cite{kupiec1992robust} applied Hidden Markov Models (HMMs) to POS tagging, modeling the tagging task as a sequence labeling problem. Brill \cite{brill1995transformation} introduced a transformation-based learning approach that combined rule induction with error-driven updates, achieving competitive results with interpretable rules.

Machine learning continued to drive innovation. Ratnaparkhi \cite{ratnaparkhi1996maximum} introduced a Maximum Entropy tagger that incorporated contextual features in a flexible framework. Conditional Random Fields (CRFs), as formalized by Lafferty et al. \cite{lafferty2001conditional}, improved sequence labeling by modeling entire sequences of tags jointly, overcoming some of the limitations of HMMs.

The 2000s also saw the development of more feature-rich models. Toutanova et al. \cite{toutanova2003feature} proposed a cyclic dependency network that leveraged linguistic features like prefixes and suffixes, improving accuracy on benchmark corpora.

With the rise of deep learning in the 2010s, POS tagging transitioned into the neural era. Huang et al. \cite{huang2015bidirectional} demonstrated that bidirectional LSTM-CRF models could outperform traditional methods without the need for hand-engineered features. Akbik et al. \cite{akbik2018contextual} introduced contextual string embeddings, capturing rich character-level and word-level representations. The introduction of large pretrained language models like BERT \cite{devlin2019bert} further advanced the state of the art in POS tagging, especially when fine-tuned for sequence labeling tasks.

Finally, the push toward multilingual and cross-lingual tagging led to the creation of Universal Dependencies (UD) \cite{nivre2016universal}, which provided standardized POS tags and treebanks across dozens of languages, enabling consistent evaluation and transfer learning techniques.

Together, these developments reflect the broader trajectory of NLP: from hand-crafted systems to data-driven statistical models, and now to powerful, general-purpose neural architectures.


A key development in standardized syntactic annotation came with \citet{petrov2012universal}'s introduction of a universal part-of-speech tagset, mapping diverse language-specific tag inventories to a shared 12-tag schema to support multilingual and cross-lingual syntactic processing. This standardization has been crucial for evaluating POS tagging performance across languages and models.

\subsection{Milestones in Phrase Structure Parsing}

Phrase structure parsing, also known as constituency parsing, is a foundational task in natural language processing (NLP) that aims to recover the hierarchical phrase-based structure of a sentence. Over the decades, the field has transitioned from symbolic rule-based systems to statistical models and, more recently, to neural and transformer-based architectures. This subsection traces the history of these developments through a set of landmark papers that shaped the trajectory of research in this area.

The theoretical basis for phrase structure parsing originated in the field of linguistics, with Chomsky's \textit{Syntactic Structures} \cite{chomsky1957syntactic} introducing the concept of generative grammars and context-free rules. These ideas laid the groundwork for early parsers, which relied on hand-crafted rules but struggled with ambiguity and scale.

The introduction of the Penn Treebank \cite{marcus1993building} marked a pivotal moment for empirical NLP. This large annotated corpus enabled the training and evaluation of statistical models, moving the field beyond rule-based systems. Leveraging this resource, Collins \cite{collins1997three} developed one of the first successful lexicalized probabilistic parsers, incorporating headwords into probabilistic context-free grammars (PCFGs) to improve disambiguation. In parallel, Charniak \cite{charniak1997statistical} proposed a PCFG-based parser that further demonstrated the viability of statistical parsing using rich lexical statistics.

As parsing performance plateaued under purely generative models, researchers shifted toward discriminative approaches. Collins and Koo \cite{collins2005discriminative} introduced reranking techniques, using discriminative models to select the best parse from a list of candidates generated by a baseline parser. Around the same time, McClosky et al. \cite{mcclosky2006effective} showed how semi-supervised learning via self-training could boost parsing performance by leveraging large amounts of unlabeled text, a strategy that prefigured later trends in unsupervised and transfer learning.

The next paradigm shift came with the rise of neural networks. Socher et al. \cite{socher2013parsing} pioneered the use of recursive neural networks for parsing, modeling the compositional structure of phrases directly in a vector space. This work demonstrated that deep learning could encode syntactic structure in a meaningful and learnable way. Soon after, Vinyals et al. \cite{vinyals2015grammar} reframed parsing as a sequence-to-sequence problem, using LSTMs to generate trees directly from input sentences—an early sign of the growing convergence between parsing and general-purpose sequence modeling.

The introduction of self-attention mechanisms led to a leap in performance and efficiency. Kitaev and Klein \cite{kitaev2018constituency} proposed a parser based on a self-attentive encoder that achieved state-of-the-art results without recurrent networks. They later extended their model with multilingual pretraining \cite{kitaev2019multilingual}, showing that transformer-based architectures like BERT could be effectively adapted for constituency parsing across languages.

Finally, Mrini et al. \cite{mrini2020rethinking} revisited the design of transformer architectures with an eye toward interpretability and efficiency, refining attention mechanisms specifically for parsing tasks. Their work represents a broader trend in NLP: optimizing pretrained models not just for raw performance, but also for transparency and adaptability.

These papers collectively chart the field’s evolution from hand-crafted symbolic systems to flexible, data-driven, and multilingual parsing solutions. Each advance reflects broader methodological shifts in NLP and has influenced not only syntactic parsing but also downstream tasks like machine translation, information extraction, and question answering.


\subsection{Dependency Parsing}

Moving from shallow to deep syntax, dependency parsing remains a more challenging task for LLMs. Traditional approaches rely heavily on supervised learning and task-specific architectures. However, \citet{hromei2024udeppllama}'s U-DepPLLaMA reframes dependency parsing as a sequence-to-sequence task, where the model learns to generate bracketed representations of dependency trees directly from raw sentences. Without relying on task-specific architecture, U-DepPLLaMA achieves competitive performance across 50 Universal Dependencies treebanks in 26 languages, demonstrating that autoregressive LLMs can internalize structured syntactic representations through fine-tuning.

The challenge of multilingual dependency parsing has been a longstanding focus of research. \citet{mcdonald2011multi} demonstrated that delexicalized dependency parsers could be effectively transferred across languages, showing that syntactic structure could be learned independently of lexical content. This insight has influenced modern approaches to cross-lingual parsing, including recent work with LLMs.

\paragraph{The CoNLL Format.} The CoNLL shared tasks have played a central role in advancing dependency parsing by establishing standardized formats and evaluation frameworks. The original CoNLL-X shared task \citep{buchholz2006conll} introduced multilingual dependency treebanks in a unified format. This was followed by CoNLL 2007 \citep{nivre2007conll}, which expanded language coverage and improved annotation consistency.

A pivotal development in representation came with \citet{de2006generating}'s Stanford Typed Dependencies, which provided a syntactically and semantically motivated dependency scheme. These efforts influenced the Universal Dependencies (UD) project \citep{nivre2016universal}, which today serves as the primary framework for evaluating dependency parsers in the CoNLL-U format.

\subsection{Zero-Shot Dependency Parsing}

Recent research has explored whether LLMs like ChatGPT can perform dependency parsing in a zero-shot setting. \citet{lin2023chatgpt} investigated whether ChatGPT could generate dependency trees when prompted directly in CoNLL format. While the model showed some emergent syntactic awareness and produced partially valid parses, performance remained far below that of supervised parsers, with LAS scores of only 28.61 on PTB and 11.93 on CTB5. Outputs also required significant post-processing due to formatting inconsistencies and labeling errors. Other LLMs (e.g., Vicuna, HuggingChat) failed to generate valid output even with one-shot examples.

Thus, while promising, zero-shot dependency parsing with ChatGPT is not yet reliable. However, the observed generalization suggests potential for supporting downstream linguistic analysis.

\subsection{Semantic Parsing and Compositional Semantics}

The relationship between syntactic and semantic parsing has long been a central concern in NLP. Early work by \citet{zelle1996} introduced one of the first data-driven semantic parsers, mapping natural language to Prolog queries via inductive logic programming. This line of research evolved through grammar-based and statistical approaches.

\citet{zettlemoyer2005} proposed a method for learning Combinatory Categorial Grammars (CCGs) from utterance-logical form pairs. This was extended by \citet{zettlemoyer2007online}, who introduced an online learning approach for more scalable parsing. \citet{bos2004} demonstrated wide-coverage semantic representations with CCGs, and \citet{kwiatkowski2010} showed how probabilistic grammars could be induced from logical forms.

An alternative view came from \citet{wong2006learning}, who framed semantic parsing as a machine translation problem using synchronous grammars. \citet{wong2007} incorporated lambda calculus to enhance compositional expressivity.

Later work moved toward weak supervision. \citet{clarke2010} learned from question-answer pairs instead of logical forms, introducing supervision via task outcomes. \citet{goldwasser2011confidence} and \citet{artzi2011} leveraged weak signals and bootstrapping from dialogue data. A major step came with \citet{liang2013learning}'s Dependency-Based Compositional Semantics (DCS), which induced semantic parsers from QA supervision using a dependency-style formalism.

This body of work underscores how structured syntax informs compositional semantics, laying the groundwork for evaluating how LLMs internalize both levels of linguistic structure.

Recent work has further explored LLMs' capabilities in semantic parsing. \citet{liu2023llm} investigate the zero-shot capabilities of GPT-3.5 and GPT-4 in semantic parsing tasks, including Text-to-SQL and AMR parsing. They find that while both models show strong performance in zero-shot scenarios, GPT-4 significantly outperforms GPT-3.5, particularly in complex parsing tasks. Their work highlights the importance of model scale and architecture in achieving robust semantic parsing capabilities.

\citet{sun-etal-2023-battle} conduct a comprehensive comparison of six prominent LLMs (Dolly, LLaMA, Vicuna, Guanaco, Bard, and ChatGPT) in Text-to-SQL parsing across nine benchmark datasets. Their systematic evaluation using five different prompting strategies reveals that while open-source models have made significant progress through instruction-tuning, they still lag behind closed-source models like GPT-3.5. This finding underscores the ongoing challenges in developing open-source alternatives that can match the performance of commercial LLMs in complex parsing tasks.

\citet{liu2023llm} also examine the impact of different prompting strategies on parsing performance, finding that structured prompts that explicitly guide the model through the parsing process tend to yield better results than simple task descriptions. They also investigate the role of in-context learning, showing that carefully selected examples can significantly improve parsing accuracy, particularly for complex queries.

\subsection{Historical Evolution and State of the Art in Text-to-SQL}

The task of translating natural language questions into executable SQL queries---commonly referred to as \emph{text-to-SQL}---has undergone significant transformation over the past decade. This section outlines the major milestones and the current landscape of text-to-SQL research, focusing on the interplay between evolving model architectures and data resources, and separating proprietary and open-source approaches.

\subsubsection{Early Systems and the Rise of Benchmarks}

Initial efforts in text-to-SQL were rule-based and constrained to narrow domains. The advent of neural networks enabled more flexible models. \citet{zhong2017seq2sql} introduced \texttt{Seq2SQL}, an early neural approach using reinforcement learning, marking the shift toward end-to-end learning.

The release of the \texttt{Spider} dataset by \citet{yu2018spider} was a critical turning point. Designed for complex, cross-domain queries, \texttt{Spider} quickly became the de facto benchmark for evaluating generalization in text-to-SQL systems. Its influence extended to shaping evaluation metrics and driving architectural innovations.

\subsubsection{Advances through Pretrained Language Models}

Following the success of general-purpose pre-trained language models (PLMs), researchers began leveraging them for semantic parsing. Techniques such as schema linking and representation learning, exemplified in \texttt{RESDSQL} by \citet{li2023resdsql}, pushed the performance of PLM-based systems on \texttt{Spider} and similar datasets.

Further, \citet{deng2021structure} demonstrated that grounding pretraining in schema and table structures could substantially improve performance, introducing structure-aware PLM training. These approaches, however, were limited by the fixed capacity of PLMs and required task-specific tuning.

\subsubsection{Transition to Large Language Models (LLMs)}

The recent emergence of Large Language Models (LLMs) has transformed the text-to-SQL landscape. Studies such as \citet{rajkumar2022evaluating} and \citet{pourreza2023dinsql} evaluated LLMs like GPT-3 and GPT-4 in zero- and few-shot regimes, showing strong performance without explicit training. \texttt{DIN-SQL} in particular introduced decomposition and self-correction to improve in-context learning (ICL).

Despite these advances, proprietary LLMs pose challenges for reproducibility and data privacy. To address this, the community has turned to open-source LLMs and training pipelines.

\subsubsection{Open-Source LLMs and Training Pipelines}

Recent work by \citet{li2024codes} proposed \texttt{CodeS}, a pre-training and fine-tuning framework specifically designed for SQL generation. It builds upon code-specific language models like StarCoder and incorporates SQL-augmented corpora for improved domain alignment.

Complementarily, \citet{hong2024knowledge} proposed \texttt{Knowledge-to-SQL}, an expert-augmented framework using fine-tuned models to inject domain knowledge and schema-aware reasoning, showing promising results on realistic and adversarial datasets.

\subsubsection{Benchmarking the Landscape}

A comprehensive benchmark comparison by \citet{gao2024benchmark} contrasted both proprietary and open-source LLMs across multiple datasets, including \texttt{Spider}, \texttt{BIRD}, and \texttt{Spider-Realistic}. Their findings illustrate a performance gap between closed and open models but highlight the rapid improvement of open-source alternatives through fine-tuning and prompt engineering.

\subsubsection{Current Challenges and Future Directions}

While LLMs show remarkable flexibility, the gap between controlled benchmarks and real-world deployment remains. Robustness to vague queries, schema complexity, and low-resource domains are active areas of research. Models like \texttt{SQLNet}~\cite{xu2017sqlnet} and newer decomposition-based methods suggest that combining symbolic reasoning with generative models may offer a path forward.

As the field continues to mature, there is a growing emphasis on interpretability, cost-efficiency, and privacy---challenges especially pertinent for industrial deployment. The interplay between open-source LLMs and domain-adapted fine-tuning pipelines marks a promising direction for creating practical, general-purpose natural language interfaces to databases.

\paragraph{Conclusion.}
Taken together, these studies highlight the expanding capacity of LLMs to perform syntactic and semantic analysis across languages and tasks. From POS tagging to deep compositional semantics, LLMs increasingly demonstrate internalized linguistic structure. Yet consistent challenges in structured prediction—especially for multilingual and zero-shot scenarios—indicate that traditional parsing insights remain essential. This review sets the stage for our own investigation into [insert your focus here], which further explores the interface between LLM prompting, structure induction, and linguistic generalization.

