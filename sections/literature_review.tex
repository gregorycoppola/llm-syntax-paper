% Literature review section content will go here 
Recent work has explored the ability of large language models (LLMs) to serve as syntactic annotators, generating linguistic labels such as part-of-speech (POS) tags, dependency trees, and constituency parses. These models can operate in zero-shot, few-shot, or fine-tuned settings, and have shown varying degrees of success depending on the syntactic task, language, and model scale.

Despite the rapid progress in language modeling, \citet{hu2020systematic} present a large-scale evaluation of syntactic generalization that reveals several striking limitations in current neural language models. Contrary to expectations, perplexity—a standard benchmark for language modeling—was poorly correlated with syntactic generalization ability: models with lower perplexity often performed worse on syntax-targeted tests, undermining the assumption that broad coverage leads to deeper linguistic competence. Surprisingly, standard LSTM architectures, even when trained on tens of millions of tokens, failed to robustly learn basic syntactic phenomena such as subject–verb agreement, and performed at near-chance levels on long-distance dependencies and center-embedding constructions. While Transformer-based models like GPT-2 achieved stronger performance, the study showed that much of this success could be attributed to preprocessing artifacts such as subword tokenization from large-scale pretraining, rather than architectural advantages per se. Perhaps most notably, even the best models consistently failed on syntactic licensing tests, such as those involving negative polarity items—phenomena that humans acquire early and handle reliably. These results suggest that many linguistic generalizations that are trivial for humans remain surprisingly elusive for even the most capable neural language models.

\subsection{POS Tagging and Shallow Syntax}

A key development in standardized syntactic annotation came with \citet{petrov2012universal}'s introduction of a universal part-of-speech tagset, mapping diverse language-specific tag inventories to a shared 12-tag schema to support multilingual and cross-lingual syntactic processing. This standardization has been crucial for evaluating POS tagging performance across languages and models.

\citet{blevins2023llmpos} examined GPT-3 and GPT-J models on POS tagging, NER, and chunking tasks. Few-shot prompting with GPT-J 6B reached 79\% accuracy for POS tagging, with GPT-3 (175B) scoring slightly lower. While this lags behind traditional supervised taggers (\textasciitilde96\%), it demonstrates the models' internalized syntactic knowledge.

\citet{lai2023chatgptpos} used zero-shot prompting with ChatGPT across 17 languages. Using prompts in either English or the target language, they achieved average POS accuracy of 84--85\%, outperforming fine-tuned XLM-R in most languages.

\citet{machado2024portpos} evaluated GPT-3, LLaMA-7B, and a Brazilian Portuguese LLM on POS tagging. With 10-shot prompts, GPT-3 achieved 90\% accuracy, significantly outperforming the smaller models.

\subsection{Agentic Prompting for Translation Tasks}

Building on a growing body of work that explores large language models (LLMs) as agentic linguistic annotators \citep[e.g.,][]{lai2023chatgptpos, blevins2023llmpos}, \citet{jiao2024gradable} investigate the extent to which LLMs can behave as adaptive agents in machine translation tasks. Rather than retraining or fine-tuning models, their approach leverages prompt engineering to steer ChatGPT's translation outputs across a continuum of linguistic complexity and context awareness.

They introduce the \textbf{T3S taxonomy}, which categorizes translation prompts into five levels of increasing detail and linguistic guidance. The dimensions of this taxonomy—expression type, translation style, part-of-speech (POS) information, and few-shot examples—mirror the types of scaffolding used in syntactic annotation tasks. At higher levels (e.g., Level 4), prompts ask ChatGPT to revise and proofread its own output, encouraging self-monitoring behavior akin to agentic reasoning.

Experimental results on the FLORES-101 Chinese--English corpus show a strong correlation between prompt complexity and translation quality, as measured by BLEU, ROUGE, and human evaluation. Notably, Level 4 prompts outperform even GPT-4's zero-shot baseline. These findings suggest that translation quality can be significantly improved through carefully structured, linguistically informed prompts—demonstrating the capacity of LLMs to act as agentic language processors even in high-level tasks like translation.

\subsection{Constituency Parsing}

\citet{bai2023llmconst} tested GPT-3.5, GPT-4, and LLaMA models on English constituency parsing. GPT-4 achieved an F1 of 73 with five-shot prompting, while open-weight LLaMA models scored below 30 F1.

\citet{tian2024chunkprompt} introduced a chunk-then-parse prompting approach using chain-of-thought (CoT) reasoning. This improved GPT-4's parsing quality on English and Chinese datasets, recovering deeper tree structures.

\subsection{The CoNLL Task and Format}

The CoNLL shared tasks have been central to advancing dependency parsing research, establishing standardized formats and evaluation frameworks. The original CoNLL-X shared task \citep{buchholz2006conll} introduced multilingual dependency treebanks in a standardized format, marking a major milestone in the field. This was followed by the CoNLL 2007 shared task \citep{nivre2007conll}, which expanded the language coverage and improved annotation consistency.

A key development in dependency representation came with \citet{de2006generating}'s introduction of the Stanford Typed Dependencies, a syntactically and semantically motivated dependency representation that enabled consistent, cross-domain analyses by converting phrase structure trees into interpretable grammatical relations. This work influenced the development of modern dependency formats and annotation schemes.

The modern Universal Dependencies (UD) project \citep{nivre2016universal} represents the current state of the art in dependency treebank development. UD provides a consistent annotation scheme across languages using the CoNLL-U format, which extends the original CoNLL format with additional features like morphological attributes and enhanced dependencies. This standardized format has become the de facto standard for evaluating dependency parsers, including recent work with LLMs.

\subsection{Semantic Parsing and Compositional Semantics}

The relationship between syntactic and semantic parsing has been a central theme in natural language processing research. Early work by \citet{zelle1996} introduced one of the first data-driven semantic parsers using inductive logic programming to map natural language to Prolog queries, laying foundational work for learning from logical forms. This line of research evolved through several key developments in grammar-based and statistical approaches.

\citet{zettlemoyer2005} proposed a method for learning Combinatory Categorial Grammars (CCGs) from utterance-logical form pairs, which became a cornerstone in grammar induction for semantic parsing. This work was extended by \citet{zettlemoyer2007online}, who developed an online learning approach for relaxed CCGs, enabling more efficient and scalable semantic parsing from logical forms. \citet{bos2004} demonstrated that CCG parsing could provide wide-coverage semantic representations, while \citet{kwiatkowski2010} showed how probabilistic CCG grammars could be induced from logical forms with higher-order unification.

An alternative approach emerged with \citet{wong2006learning}, who modeled semantic parsing as a statistical machine translation problem using synchronous grammars to align natural and formal languages. This work was further developed by \citet{wong2007}, who incorporated lambda calculus into synchronous grammar-based semantic parsers, allowing for greater compositional expressivity.

More recent work has explored interactive and weakly supervised approaches to semantic parsing. \citet{clarke2010} pioneered weakly supervised semantic parsing by learning from question-answer pairs instead of logical forms, introducing the paradigm of using the world's response to supervise learning. \citet{goldwasser2011confidence} introduced a confidence-driven unsupervised approach that leverages weak signals to build structure without annotated logical forms. \citet{artzi2011} proposed a bootstrapping method for learning semantic parsers from dialogues without annotated logical forms, demonstrating effective learning from conversational supervision.

A significant advancement came with \citet{liang2013learning}'s Dependency-Based Compositional Semantics (DCS), which developed a formalism and learning framework for inducing semantic parsers from question-answer supervision. This approach achieved near state-of-the-art results with less supervision, demonstrating the potential for more efficient semantic parsing through dependency-based representations.

This rich history of semantic parsing research provides important context for understanding the current capabilities and limitations of LLMs in syntactic annotation tasks. The success of early semantic parsers in mapping between syntactic and semantic representations suggests that LLMs may be able to leverage their internalized knowledge of both levels of linguistic structure.

\subsection{Dependency Parsing}

While constituency parsing has shown promising results with LLMs, dependency parsing remains a challenging task. Current approaches often require fine-tuning or specialized prompting strategies to achieve competitive performance. A notable exception is \citet{hromei2024udeppllama}'s U-DepPLLaMA, which reframes dependency parsing as a sequence-to-sequence task, where the model learns to generate bracketed representations of dependency trees directly from raw sentences. Despite the absence of task-specific architecture, U-DepPLLaMA achieves competitive performance across 50 Universal Dependencies treebanks in 26 languages, and shows robustness to sentence length and complexity. This work demonstrates that autoregressive LLMs can learn structured syntactic representations purely through fine-tuning.

The challenge of multilingual dependency parsing has been a longstanding focus of research. \citet{mcdonald2011multi} demonstrated that delexicalized dependency parsers could be effectively transferred across languages, showing that syntactic structure could be learned independently of lexical content. This insight has influenced modern approaches to cross-lingual parsing, including recent work with LLMs.

\subsection{Zero-Shot Dependency Parsing}

Recent research has explored the potential for large language models (LLMs), such as ChatGPT, to perform dependency parsing in a zero-shot setting. While traditional approaches rely on task-specific architectures and supervised training, \citet{lin2023chatgpt} investigate whether ChatGPT can generate syntactic dependency trees when prompted directly in CoNLL format.

Their findings indicate that ChatGPT exhibits some emergent syntactic awareness, and can produce partially valid dependency parses for English and Chinese. However, its performance remains far below that of supervised parsers, with LAS scores of only 28.61 on PTB and 11.93 on CTB5. Furthermore, outputs often require extensive post-processing due to format inconsistencies, token omissions, or labeling errors. Other LLMs (e.g., Vicuna, HuggingChat) fail to generate valid output even when given one-shot examples.

Thus, while ChatGPT shows promise in demonstrating linguistic generalization, it does not function as a reliable one-shot dependency parser. Instead, it reflects an early-stage capability that may support downstream linguistic analysis rather than robust structured prediction.

\subsection{Summary of Methods and Trends}

\begin{table}[ht]
\centering
\begin{tabular}{p{3.3cm} p{2cm} p{2cm} p{2.5cm} p{3.5cm}}
\toprule
\textbf{Study} & \textbf{Model} & \textbf{Task} & \textbf{Method} & \textbf{Result / Remark} \\
\midrule
Blevins et al. (2023) & GPT-3, GPT-J & POS, Chunk & Few-shot & Up to 79\% POS (GPT-J); below SOTA \\
Lai et al. (2023) & ChatGPT & POS (UD) & Zero-shot & Avg. 85\% accuracy (17 langs) \\
Machado \& Ruiz (2024) & GPT-3 & POS (PT-BR) & Few-shot & 90\% POS accuracy \\
Jiao et al. (2024) & ChatGPT & Translation & T3S Taxonomy & Level 4 prompts beat GPT-4 baseline \\
Bai et al. (2023) & GPT-4, LLaMA & Constituency & Few-shot & GPT-4: 73 F1; LLaMA: $<$30 F1 \\
Tian et al. (2024) & GPT-4, GPT-3.5 & Constituency & Chunk + CoT & Improved tree depth and quality \\
Hromei et al. (2024) & U-DepPLLaMA & Dependency & Fine-tuned & Competitive on 50 UD treebanks \\
Lin et al. (2023) & ChatGPT & Dependency & Zero-shot & 28.61 LAS (PTB), 11.93 LAS (CTB5) \\
\bottomrule
\end{tabular}
\caption{LLMs for syntactic annotation: task, model, method, and performance summary.}
\label{tab:llm_syntax}
\end{table}

These studies demonstrate that LLMs can approximate human-level syntactic annotation, particularly for POS tagging. However, performance in parsing (especially full dependency trees) remains variable without fine-tuning. Prompt design, instruction tuning, and model scale all contribute to improved results, and multilingual evaluation remains a critical benchmark for future progress.


