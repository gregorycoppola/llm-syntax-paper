% Literature review section content will go here
Recent work has explored the ability of large language models (LLMs) to serve as syntactic annotators, generating linguistic labels such as part-of-speech (POS) tags, dependency trees, and constituency parses. These models operate in zero-shot, few-shot, or fine-tuned settings and exhibit varying levels of success depending on the task, language, and model scale.

However, despite these advances, fundamental questions remain about the linguistic competence of LLMs. For instance, \citet{hu2020systematic} present a large-scale evaluation of syntactic generalization that reveals several striking limitations in current neural language models. Contrary to expectations, perplexity—a standard benchmark for language modeling—was poorly correlated with syntactic generalization ability: models with lower perplexity often performed worse on syntax-targeted tests, undermining the assumption that broad coverage leads to deeper linguistic competence. Standard LSTM architectures, even when trained on tens of millions of tokens, failed to robustly learn basic syntactic phenomena such as subject–verb agreement, and performed at near-chance levels on long-distance dependencies and center-embedding constructions. Transformer-based models like GPT-2 achieved stronger performance, but much of this success was attributed to preprocessing artifacts such as subword tokenization rather than architectural advantages. Perhaps most notably, even the best models consistently failed on syntactic licensing tests, such as those involving negative polarity items—phenomena that humans acquire early and handle reliably. These results suggest that many linguistic generalizations that are trivial for humans remain surprisingly elusive for even the most capable neural language models.

\subsection{Foundations of Generative Linguistics and Formal Semantics}

The development of generative linguistics in the second half of the 20th century marked a dramatic shift in the scientific study of language, emphasizing the formal structure of syntax and its relationship to meaning. This subsection highlights five foundational works that shaped this intellectual movement and laid the groundwork for modern syntactic and semantic theory.

\paragraph{Chomsky (1957): \textit{Syntactic Structures}.}
This work introduced the concept of a \emph{generative grammar}—a formal system capable of generating the infinite set of grammatical sentences in a language. Chomsky proposed a set of phrase structure rules and transformational rules, arguing that syntactic theory should be precise, explicit, and formally rigorous. The book also introduced the notion of deep structure and raised fundamental questions about the inadequacy of finite-state models for capturing human syntax. It effectively launched the generative paradigm and established syntax as a central object of linguistic inquiry.

\paragraph{Chomsky (1965): \textit{Aspects of the Theory of Syntax}.}
In this follow-up, Chomsky significantly extended his theoretical framework by distinguishing between \emph{competence} (the idealized knowledge of language) and \emph{performance} (actual language use). He refined the concept of deep vs. surface structure and introduced the T-model architecture of grammar. The work also proposed the theory of \emph{universal grammar}—a set of innate principles and parameters shared by all human languages—which became a cornerstone of subsequent research in generative syntax and language acquisition.

\paragraph{Katz and Fodor (1963): \textit{The Structure of a Semantic Theory}.}
This influential paper represented one of the earliest attempts to systematically integrate semantics into the generative framework. Katz and Fodor proposed a decompositional view of lexical meaning, where word senses are defined in terms of bundles of semantic features. Their model attempted to link syntactic structure to semantic interpretation, setting the stage for formal approaches to the syntax–semantics interface. Though later surpassed by model-theoretic semantics, their work was instrumental in establishing semantics as a formal, computationally tractable component of linguistic theory.

\paragraph{Jackendoff (1972): \textit{Semantic Interpretation in Generative Grammar}.}
Jackendoff extended the generative program by proposing a more elaborate mapping from syntactic structure to semantic interpretation. He introduced the idea of multiple levels of representation—syntactic, semantic, and phonological—and emphasized their interaction. His framework helped clarify the architecture of grammar and provided early formulations of what would become \emph{conceptual semantics}, influencing both generative and cognitive approaches to meaning.

\paragraph{Montague (1970): \textit{English as a Formal Language}.}
Montague revolutionized semantic theory by showing that natural languages could be treated with the same formal rigor as logical systems. He applied tools from model theory, lambda calculus, and type theory to build compositional semantics for fragments of English. This work laid the foundation for \emph{Montague Grammar}, a highly influential framework that unified syntax and semantics. Although developed independently of the Chomskyan tradition, Montague’s work profoundly influenced generative semantics and was later integrated into transformational grammar by scholars such as Partee.

Together, these works represent the intellectual bedrock of modern linguistic theory. They established the principles of syntactic generation, formal semantic interpretation, and the architecture of linguistic competence, shaping decades of subsequent research in theoretical linguistics, psycholinguistics, and computational models of language.


\subsection{History of Part-of-Speech Tagging}

The task of part-of-speech (POS) tagging—assigning grammatical categories like noun, verb, or adjective to words in a sentence—has evolved through several major paradigms over the past decades.

Early work in the 1960s and 70s relied on rule-based approaches and manually curated lexicons. Greene and Kuno \cite{greene1963automatic} presented one of the earliest automatic tagging systems, combining dictionary lookups with hand-written disambiguation rules. The release of the Brown Corpus \cite{francis1979brown} marked a turning point by providing a large, annotated dataset for statistical analysis.

In the early 1990s, the statistical revolution in NLP led to probabilistic models becoming dominant. Kupiec \cite{kupiec1992robust} applied Hidden Markov Models (HMMs) to POS tagging, modeling the tagging task as a sequence labeling problem. Brill \cite{brill1995transformation} introduced a transformation-based learning approach that combined rule induction with error-driven updates, achieving competitive results with interpretable rules.

Machine learning continued to drive innovation. Ratnaparkhi \cite{ratnaparkhi1996maximum} introduced a Maximum Entropy tagger that incorporated contextual features in a flexible framework. Conditional Random Fields (CRFs), as formalized by Lafferty et al. \cite{lafferty2001conditional}, improved sequence labeling by modeling entire sequences of tags jointly, overcoming some of the limitations of HMMs.

The 2000s also saw the development of more feature-rich models. Toutanova et al. \cite{toutanova2003feature} proposed a cyclic dependency network that leveraged linguistic features like prefixes and suffixes, improving accuracy on benchmark corpora.

With the rise of deep learning in the 2010s, POS tagging transitioned into the neural era. Huang et al. \cite{huang2015bidirectional} demonstrated that bidirectional LSTM-CRF models could outperform traditional methods without the need for hand-engineered features. Akbik et al. \cite{akbik2018contextual} introduced contextual string embeddings, capturing rich character-level and word-level representations. The introduction of large pretrained language models like BERT \cite{devlin2019bert} further advanced the state of the art in POS tagging, especially when fine-tuned for sequence labeling tasks.

Finally, the push toward multilingual and cross-lingual tagging led to the creation of Universal Dependencies (UD) \cite{nivre2016universal}, which provided standardized POS tags and treebanks across dozens of languages, enabling consistent evaluation and transfer learning techniques.

Together, these developments reflect the broader trajectory of NLP: from hand-crafted systems to data-driven statistical models, and now to powerful, general-purpose neural architectures.


A key development in standardized syntactic annotation came with \citet{petrov2012universal}'s introduction of a universal part-of-speech tagset, mapping diverse language-specific tag inventories to a shared 12-tag schema to support multilingual and cross-lingual syntactic processing. This standardization has been crucial for evaluating POS tagging performance across languages and models.

\subsection{Milestones in Phrase Structure Parsing}

Phrase structure parsing, also known as constituency parsing, is a foundational task in natural language processing (NLP) that aims to recover the hierarchical phrase-based structure of a sentence. Over the decades, the field has transitioned from symbolic rule-based systems to statistical models and, more recently, to neural and transformer-based architectures. This subsection traces the history of these developments through a set of landmark papers that shaped the trajectory of research in this area.

The theoretical basis for phrase structure parsing originated in the field of linguistics, with Chomsky's \textit{Syntactic Structures} \cite{chomsky1957syntactic} introducing the concept of generative grammars and context-free rules. These ideas laid the groundwork for early parsers, which relied on hand-crafted rules but struggled with ambiguity and scale.

The introduction of the Penn Treebank \cite{marcus1993building} marked a pivotal moment for empirical NLP. This large annotated corpus enabled the training and evaluation of statistical models, moving the field beyond rule-based systems. Leveraging this resource, Collins \cite{collins1997three} developed one of the first successful lexicalized probabilistic parsers, incorporating headwords into probabilistic context-free grammars (PCFGs) to improve disambiguation. In parallel, Charniak \cite{charniak1997statistical} proposed a PCFG-based parser that further demonstrated the viability of statistical parsing using rich lexical statistics.

As parsing performance plateaued under purely generative models, researchers shifted toward discriminative approaches. Collins and Koo \cite{collins2005discriminative} introduced reranking techniques, using discriminative models to select the best parse from a list of candidates generated by a baseline parser. Around the same time, McClosky et al. \cite{mcclosky2006effective} showed how semi-supervised learning via self-training could boost parsing performance by leveraging large amounts of unlabeled text, a strategy that prefigured later trends in unsupervised and transfer learning.

The next paradigm shift came with the rise of neural networks. Socher et al. \cite{socher2013parsing} pioneered the use of recursive neural networks for parsing, modeling the compositional structure of phrases directly in a vector space. This work demonstrated that deep learning could encode syntactic structure in a meaningful and learnable way. Soon after, Vinyals et al. \cite{vinyals2015grammar} reframed parsing as a sequence-to-sequence problem, using LSTMs to generate trees directly from input sentences—an early sign of the growing convergence between parsing and general-purpose sequence modeling.

The introduction of self-attention mechanisms led to a leap in performance and efficiency. Kitaev and Klein \cite{kitaev2018constituency} proposed a parser based on a self-attentive encoder that achieved state-of-the-art results without recurrent networks. They later extended their model with multilingual pretraining \cite{kitaev2019multilingual}, showing that transformer-based architectures like BERT could be effectively adapted for constituency parsing across languages.

Finally, Mrini et al. \cite{mrini2020rethinking} revisited the design of transformer architectures with an eye toward interpretability and efficiency, refining attention mechanisms specifically for parsing tasks. Their work represents a broader trend in NLP: optimizing pretrained models not just for raw performance, but also for transparency and adaptability.

These papers collectively chart the field’s evolution from hand-crafted symbolic systems to flexible, data-driven, and multilingual parsing solutions. Each advance reflects broader methodological shifts in NLP and has influenced not only syntactic parsing but also downstream tasks like machine translation, information extraction, and question answering.


\subsection{Historical Milestones in Statistical Dependency Parsing}

Dependency parsing aims to identify syntactic relationships between words in a sentence, typically in the form of directed, labeled graphs where each word depends on a syntactic head. Compared to phrase structure parsing, dependency representations are often more suitable for morphologically rich and free word order languages, making them a popular choice in multilingual and applied NLP tasks. Over the past few decades, the field has undergone major methodological shifts—moving from rule-based systems to machine learning and, more recently, to deep learning and multilingual pretrained models. This subsection outlines ten influential works that have shaped the trajectory of statistical dependency parsing.

The theoretical foundation of dependency parsing can be traced back to Mel’čuk’s seminal work on dependency syntax \cite{melcuk1988dependency}, which formalized the representation of syntactic relations in a way that was both linguistically expressive and computationally viable. Although primarily theoretical, Mel’čuk’s framework laid the groundwork for many subsequent parsing systems.

A practical breakthrough came with Yamada and Matsumoto’s data-driven transition-based parser \cite{yamada2003statistical}, which modeled parsing as a sequence of decisions made by a classifier—specifically a support vector machine. Their approach pioneered the use of machine learning in dependency parsing and inspired a generation of transition-based parsers.

In contrast, McDonald et al. introduced a graph-based alternative to dependency parsing \cite{mcdonald2005online}, treating the task as finding the maximum spanning tree over possible head-dependent arcs. This approach, trained with a structured perceptron, offered global optimization and quickly became a strong baseline. McDonald and Pereira \cite{mcdonald2006online} extended this work by incorporating higher-order features such as sibling and grandparent relationships, thereby improving parsing accuracy while maintaining tractability.

Concurrently, Nivre and colleagues released MaltParser \cite{nivre2006maltparser}, a flexible and widely adopted transition-based parsing toolkit that supports various parsing strategies and languages. Its configurability and speed made it a go-to solution for both academic research and practical applications.

Pushing the boundaries of expressiveness, Koo and Collins \cite{koo2010efficient} introduced an efficient third-order parser that incorporated rich structural features such as tri-siblings and grand-siblings into the graph-based framework. Their work demonstrated how higher-order relationships could improve accuracy without prohibitive computational costs.

Bridging the gap between transition- and graph-based methods, Zhang and Nivre \cite{zhang2011transition} showed that using beam search and non-local features in transition-based parsing could dramatically improve performance, allowing fast parsers to model longer-range dependencies more effectively.

The era of neural parsing began with the work of Chen and Manning \cite{chen2014fast}, who replaced hand-engineered feature templates with dense, learned representations using feedforward neural networks. This marked the first successful neural dependency parser and set the stage for deep learning approaches in syntactic analysis.

Dozat and Manning \cite{dozat2017deep} further advanced the state of the art with their biaffine parser, which combined BiLSTM encoders and attention-based scoring in a graph-based architecture. This model achieved exceptional accuracy and became the de facto baseline for modern dependency parsing.

Finally, Kondratyuk and Straka \cite{kondratyuk2019udpipe} leveraged multilingual BERT representations to build a single model capable of parsing 75 languages. This work illustrated the power of transfer learning and multilingual pretraining, solidifying dependency parsing’s place in the transformer era.

Collectively, these works reflect the field’s evolution from symbolic representations to fast transition-based systems, globally optimized graph-based models, and finally to multilingual, deep neural approaches. Each milestone introduced innovations that not only advanced parsing accuracy but also expanded the range of languages and applications dependency parsing could support.

\paragraph{The CoNLL Format.} The CoNLL shared tasks have played a central role in advancing dependency parsing by establishing standardized formats and evaluation frameworks. The original CoNLL-X shared task \citep{buchholz2006conll} introduced multilingual dependency treebanks in a unified format. This was followed by CoNLL 2007 \citep{nivre2007conll}, which expanded language coverage and improved annotation consistency.

A pivotal development in representation came with \citet{de2006generating}'s Stanford Typed Dependencies, which provided a syntactically and semantically motivated dependency scheme. These efforts influenced the Universal Dependencies (UD) project \citep{nivre2016universal}, which today serves as the primary framework for evaluating dependency parsers in the CoNLL-U format.

\subsection{Zero-Shot Dependency Parsing}

Recent research has explored whether LLMs like ChatGPT can perform dependency parsing in a zero-shot setting. \citet{lin2023chatgpt} investigated whether ChatGPT could generate dependency trees when prompted directly in CoNLL format. While the model showed some emergent syntactic awareness and produced partially valid parses, performance remained far below that of supervised parsers, with LAS scores of only 28.61 on PTB and 11.93 on CTB5. Outputs also required significant post-processing due to formatting inconsistencies and labeling errors. Other LLMs (e.g., Vicuna, HuggingChat) failed to generate valid output even with one-shot examples.

Thus, while promising, zero-shot dependency parsing with ChatGPT is not yet reliable. However, the observed generalization suggests potential for supporting downstream linguistic analysis.

\subsection{Semantic Parsing and Compositional Semantics}

The relationship between syntactic and semantic parsing has long been a central concern in NLP. Early work by \citet{zelle1996} introduced one of the first data-driven semantic parsers, mapping natural language to Prolog queries via inductive logic programming. This line of research evolved through grammar-based and statistical approaches.

\citet{zettlemoyer2005} proposed a method for learning Combinatory Categorial Grammars (CCGs) from utterance-logical form pairs. This was extended by \citet{zettlemoyer2007online}, who introduced an online learning approach for more scalable parsing. \citet{bos2004} demonstrated wide-coverage semantic representations with CCGs, and \citet{kwiatkowski2010} showed how probabilistic grammars could be induced from logical forms.

An alternative view came from \citet{wong2006learning}, who framed semantic parsing as a machine translation problem using synchronous grammars. \citet{wong2007} incorporated lambda calculus to enhance compositional expressivity.

Later work moved toward weak supervision. \citet{clarke2010} learned from question-answer pairs instead of logical forms, introducing supervision via task outcomes. \citet{goldwasser2011confidence} and \citet{artzi2011} leveraged weak signals and bootstrapping from dialogue data. A major step came with \citet{liang2013learning}'s Dependency-Based Compositional Semantics (DCS), which induced semantic parsers from QA supervision using a dependency-style formalism.

This body of work underscores how structured syntax informs compositional semantics, laying the groundwork for evaluating how LLMs internalize both levels of linguistic structure.

Recent work has further explored LLMs' capabilities in semantic parsing. \citet{liu2023llm} investigate the zero-shot capabilities of GPT-3.5 and GPT-4 in semantic parsing tasks, including Text-to-SQL and AMR parsing. They find that while both models show strong performance in zero-shot scenarios, GPT-4 significantly outperforms GPT-3.5, particularly in complex parsing tasks. Their work highlights the importance of model scale and architecture in achieving robust semantic parsing capabilities.

\citet{sun-etal-2023-battle} conduct a comprehensive comparison of six prominent LLMs (Dolly, LLaMA, Vicuna, Guanaco, Bard, and ChatGPT) in Text-to-SQL parsing across nine benchmark datasets. Their systematic evaluation using five different prompting strategies reveals that while open-source models have made significant progress through instruction-tuning, they still lag behind closed-source models like GPT-3.5. This finding underscores the ongoing challenges in developing open-source alternatives that can match the performance of commercial LLMs in complex parsing tasks.

\citet{liu2023llm} also examine the impact of different prompting strategies on parsing performance, finding that structured prompts that explicitly guide the model through the parsing process tend to yield better results than simple task descriptions. They also investigate the role of in-context learning, showing that carefully selected examples can significantly improve parsing accuracy, particularly for complex queries.

\subsection{Historical Evolution and State of the Art in Text-to-SQL}

The task of translating natural language questions into executable SQL queries---commonly referred to as \emph{text-to-SQL}---has undergone significant transformation over the past decade. This section outlines the major milestones and the current landscape of text-to-SQL research, focusing on the interplay between evolving model architectures and data resources, and separating proprietary and open-source approaches.

\subsubsection{Early Systems and the Rise of Benchmarks}

Initial efforts in text-to-SQL were rule-based and constrained to narrow domains. The advent of neural networks enabled more flexible models. \citet{zhong2017seq2sql} introduced \texttt{Seq2SQL}, an early neural approach using reinforcement learning, marking the shift toward end-to-end learning.

The release of the \texttt{Spider} dataset by \citet{yu2018spider} was a critical turning point. Designed for complex, cross-domain queries, \texttt{Spider} quickly became the de facto benchmark for evaluating generalization in text-to-SQL systems. Its influence extended to shaping evaluation metrics and driving architectural innovations.

\subsubsection{Advances through Pretrained Language Models}

Following the success of general-purpose pre-trained language models (PLMs), researchers began leveraging them for semantic parsing. Techniques such as schema linking and representation learning, exemplified in \texttt{RESDSQL} by \citet{li2023resdsql}, pushed the performance of PLM-based systems on \texttt{Spider} and similar datasets.

Further, \citet{deng2021structure} demonstrated that grounding pretraining in schema and table structures could substantially improve performance, introducing structure-aware PLM training. These approaches, however, were limited by the fixed capacity of PLMs and required task-specific tuning.

\subsubsection{Transition to Large Language Models (LLMs)}

The recent emergence of Large Language Models (LLMs) has transformed the text-to-SQL landscape. Studies such as \citet{rajkumar2022evaluating} and \citet{pourreza2023dinsql} evaluated LLMs like GPT-3 and GPT-4 in zero- and few-shot regimes, showing strong performance without explicit training. \texttt{DIN-SQL} in particular introduced decomposition and self-correction to improve in-context learning (ICL).

Despite these advances, proprietary LLMs pose challenges for reproducibility and data privacy. To address this, the community has turned to open-source LLMs and training pipelines.

\subsubsection{Open-Source LLMs and Training Pipelines}

Recent work by \citet{li2024codes} proposed \texttt{CodeS}, a pre-training and fine-tuning framework specifically designed for SQL generation. It builds upon code-specific language models like StarCoder and incorporates SQL-augmented corpora for improved domain alignment.

Complementarily, \citet{hong2024knowledge} proposed \texttt{Knowledge-to-SQL}, an expert-augmented framework using fine-tuned models to inject domain knowledge and schema-aware reasoning, showing promising results on realistic and adversarial datasets.

\subsubsection{Benchmarking the Landscape}

A comprehensive benchmark comparison by \citet{gao2024benchmark} contrasted both proprietary and open-source LLMs across multiple datasets, including \texttt{Spider}, \texttt{BIRD}, and \texttt{Spider-Realistic}. Their findings illustrate a performance gap between closed and open models but highlight the rapid improvement of open-source alternatives through fine-tuning and prompt engineering.

\subsubsection{Current Challenges and Future Directions}

While LLMs show remarkable flexibility, the gap between controlled benchmarks and real-world deployment remains. Robustness to vague queries, schema complexity, and low-resource domains are active areas of research. Models like \texttt{SQLNet}~\cite{xu2017sqlnet} and newer decomposition-based methods suggest that combining symbolic reasoning with generative models may offer a path forward.

As the field continues to mature, there is a growing emphasis on interpretability, cost-efficiency, and privacy---challenges especially pertinent for industrial deployment. The interplay between open-source LLMs and domain-adapted fine-tuning pipelines marks a promising direction for creating practical, general-purpose natural language interfaces to databases.

\paragraph{Conclusion.}
Taken together, these studies highlight the expanding capacity of LLMs to perform syntactic and semantic analysis across languages and tasks. From POS tagging to deep compositional semantics, LLMs increasingly demonstrate internalized linguistic structure. Yet consistent challenges in structured prediction—especially for multilingual and zero-shot scenarios—indicate that traditional parsing insights remain essential. This review sets the stage for our own investigation into [insert your focus here], which further explores the interface between LLM prompting, structure induction, and linguistic generalization.

\subsection{LLMs as Parsing Augmentors}

While large language models (LLMs) have shown promise in zero-shot and few-shot syntactic and semantic parsing, their ability to disambiguate lexical meaning and generalize to unseen concepts remains limited—especially in structured parsing tasks requiring precise, graph-based outputs. A recent line of work explores whether external knowledge sources can enhance LLM performance by augmenting their inputs with structured lexical information. One of the most promising developments in this area is the \textbf{RASP} framework (\textit{Retrieval-Augmented Semantic Parsing}) proposed by \citet{zhang2024rasp}.

RASP addresses a core challenge in neural semantic parsing: the tendency of LLMs to rely on heuristics, such as always predicting the most frequent sense of a word when encountering ambiguous or out-of-distribution concepts. This behavior often leads to ``lucky guesses'' and degrades the model's ability to robustly disambiguate word meanings in open-domain settings. RASP tackles this problem by injecting \textit{retrieved lexical glosses}—specifically from WordNet—into the model input, thereby grounding the model’s output in explicit semantic context.

The method operates in two stages: a retrieval step and a generation step. During retrieval, all plausible WordNet synsets (senses) for each content word in the input are gathered, along with their glosses. These glosses are concatenated and prepended to the input text in a structured format. Then, a decoder-only LLM (e.g., Mistral, Gemma, Qwen, LLaMA) is prompted with this enriched input to generate Discourse Representation Structures (DRSs) that align with the correct word senses. Formally, this integrates two probabilities: \( p(o' \mid x) \) for retrieving relevant glosses, and \( p_{\text{decoder-only}}(o \mid x, o') \) for generating the output conditioned on both the original input and the retrieved information.

\paragraph{Experimental Results.} The RASP framework was evaluated on the Parallel Meaning Bank (PMB) 5.1.0 corpus, using both standard and challenge sets. The challenge set is particularly revealing, as it contains 500 examples with at least one concept not seen during training and whose correct sense is not the first listed in WordNet—thus explicitly penalizing frequency-based heuristics.

On the standard (in-distribution) test set, RASP consistently improved performance across all tested LLMs. For instance, the Hard-SMatch score of Mistral-7B increased from 89.95 to 90.95 (+1.1\%), and the node-level F-score improved by 1.3\%. While these gains were modest, they demonstrated that lexical retrieval provides a consistent benefit even when models are already fine-tuned.

On the challenge set, however, the gains were dramatic. The overall Wu-Palmer similarity score for Gemma2-9B rose from 42.54 (without retrieval) to 70.41 (+65.6\%) with RASP. Improvements for noun disambiguation ranged from +70\% to +95\%, while verbs and modifiers also saw gains of 25--77\% and 26--43\%, respectively. These findings provide strong evidence that retrieval augmentation enables LLMs to make semantically informed predictions on unfamiliar lexical items.

\paragraph{Error Analysis.} Despite these improvements, RASP is not without limitations. The authors identify three primary sources of residual errors: (1) gloss ambiguity, where multiple senses have very similar definitions (e.g., \textit{alarm.v.01} vs. \textit{alarm.v.02}); (2) insufficient context in the input sentence; and (3) limited coverage in WordNet itself, which excludes many domain-specific or novel terms. Nonetheless, the authors argue that the retrieval process can be seen as a lightweight plug-in that enhances the semantic fidelity of LLMs without requiring extensive architectural changes.

\paragraph{Significance.} RASP demonstrates a powerful and general principle: LLMs can be significantly enhanced when paired with structured lexical knowledge in context. Rather than relying solely on scale or additional training data, RASP leverages existing resources like WordNet to bridge the gap between LLM fluency and semantic precision. This approach is especially valuable in domains where generalization to rare or out-of-vocabulary concepts is critical.

These findings are not only state-of-the-art for semantic parsing with Discourse Representation Structures, but they also suggest new directions for improving LLM-based parsers in other structured semantic tasks, including AMR and Text-to-SQL. Retrieval-augmented generation thus offers a promising paradigm for combining the strengths of statistical parsing and symbolic resources.

\subsection{Beyond Pretraining: Retrieval-Augmented and Structure-Aware Parsing with LLMs}

Recent work has demonstrated that large language models (LLMs) benefit not only from larger model size and better pretraining objectives, but also from the integration of external structure—be it in the form of symbolic knowledge, task-specific prompts, or compositional representations. This subsection surveys several prominent methods that extend the RASP paradigm through retrieval, structural supervision, or neuro-symbolic grounding to enhance LLM-based semantic parsing and generalization.

\paragraph{UPRISE: Universal Prompt Retrieval for Zero-Shot Tasks}

\citet{cheng2023uprise} propose \textbf{UPRISE}, a retrieval-augmented framework that improves zero-shot performance in structured prediction by retrieving task-specific prompts. Rather than relying solely on handcrafted prompt templates, UPRISE builds a bank of example prompts and retrieves the most relevant ones based on the test input using dense similarity. These retrieved prompts are appended to the model input before decoding, effectively grounding generation in in-context prior examples. UPRISE improves zero-shot accuracy on a range of tasks including semantic parsing, code generation, and text-to-SQL, and provides a flexible template-free alternative to handcrafted instruction tuning.

\paragraph{KG-BART: Injecting Knowledge Graphs into Generative Models}

\citet{liu2021kgbart} introduce \textbf{KG-BART}, a framework that incorporates subgraphs from external knowledge graphs—specifically ConceptNet—into the BART model to enhance commonsense reasoning. The model encodes these graphs alongside the input sequence, allowing the decoder to attend to both textual and symbolic representations. Although developed for commonsense QA, the approach demonstrates the value of integrating structured external knowledge into generative models. KG-BART provides evidence that grounding LLMs in symbolic context improves their semantic fidelity and robustness, particularly in low-resource or ambiguous settings.

\paragraph{StructGPT: Parsing as Structured Action}

\citet{lu2023structgpt} present \textbf{StructGPT}, a method that treats structured prediction as a sequence of programmatic actions using a domain-specific language (DSL). Rather than producing raw text, the LLM is prompted to generate parse trees or structured outputs by selecting from tool-like functions (e.g., "AddNode", "SetLabel"). This framing allows the LLM to reason in a constrained, compositional space, dramatically improving structure compliance. While not retrieval-based, StructGPT illustrates how guiding LLM output using symbolic constraints can improve precision and interpretability in parsing tasks.

\paragraph{ATLAS: Retrieval-Augmented Few-Shot Learning}

\textbf{ATLAS} \citep{izacard2024atlas} is a general-purpose retrieval-augmented generation framework that fine-tunes LLMs to perform tasks conditioned on external retrieved text segments. Unlike systems that treat retrieval as static, ATLAS jointly learns both the retriever and generator end-to-end. Although originally proposed for QA and open-domain tasks, ATLAS has since been adapted to AMR parsing and other meaning representation tasks. The key insight is that pairing LLMs with learned retrievers allows them to generalize to unseen inputs using latent semantic memory. ATLAS represents a scalable, modular approach to grounding LLMs in task-relevant knowledge.

\paragraph{Structure-Aware Pretraining for Structured Data}

\citet{li2023structure} propose \textbf{structure-aware pretraining} (SAP), which modifies the pretraining objective of LLMs to emphasize structural relationships in data such as tables and parse trees. By pretraining on structured data with hierarchical and compositional constraints, the resulting models generalize better to downstream tasks like dense retrieval and Text-to-SQL. Unlike RASP or UPRISE, this approach does not involve retrieval at inference time; instead, it biases the model during pretraining to become sensitive to structured patterns. SAP demonstrates that injecting structural inductive bias into LLM pretraining leads to more robust parsing capabilities even without fine-tuning.

\paragraph{TAX-Parser and AMS-Parser: Neuro-Symbolic Integration}

Finally, two neuro-symbolic systems—\textbf{TAX-Parser} \citep{zhang2024taxparser} and \textbf{AMS-Parser} \citep{yang2024amsparser}—offer complementary approaches to integrating structured meaning representations with neural models. TAX-Parser encodes Discourse Representation Structures (DRS) into linearized forms and trains neural models to generate these sequences with high accuracy. AMS-Parser goes further by using explicit compositional operators and symbol manipulation during inference, achieving very low ill-formed rates and strong generalization. These systems outperform many baseline LLMs on DRS parsing, especially in cases where interpretability and syntactic correctness are critical. While not retrieval-augmented, their neuro-symbolic nature aligns with the RASP philosophy of combining LLM fluency with external structure.

\paragraph{Retrieval-Augmented Semantic Parsing (RASP)}

RASP (Retrieval-Augmented Semantic Parsing) enhances large language models (LLMs) for structured semantic parsing by incorporating external lexical knowledge at inference time. Specifically, given an input sentence, RASP retrieves WordNet synsets (concepts) for content words and appends their glosses (definitions) to the model input. This extended prompt helps the model disambiguate polysemous words based on context and guides the generation of Discourse Representation Structures (DRSs).

The generation process is left-to-right, consistent with decoder-only models. Formally, the probability of generating a meaning representation \( o \) given input sentence \( x \) and retrieved concepts \( o' \) is defined as:

\[
P_{\text{RASP}}(o \mid x) = p(o' \mid x) \cdot \prod_{i=1}^{n} p_\theta(o_i \mid x, o', o_{1:i-1})
\]

This approach improves both in-distribution and out-of-distribution (OOD) generalization. On standard benchmarks from the Parallel Meaning Bank (PMB 5.1.0), RASP consistently outperforms non-augmented LLMs. For instance, the 9B-parameter Gemma2 model improved its Hard-SMatch score from 90.72 to 91.37 and its F1 node-level score from 84.67 to 86.11. 

More notably, on a challenge set designed to test OOD concept understanding, RASP nearly doubled the semantic similarity (measured by Wu-Palmer similarity) between predicted and gold concepts. Gemma2-9B, for example, improved its overall similarity score from 42.54 to 70.41. These gains were especially significant for rare noun senses, where improvements ranged from 70\% to 95\%.

RASP demonstrates that retrieval-based input augmentation can substantially enhance concept-level semantic understanding and disambiguation in LLMs, especially in low-resource or zero-shot settings.

\paragraph{LLMs for Conversational Semantic Parsing}

\cite{semler2023llmsp} benchmarked the capabilities of large language models (LLMs) for semantic parsing in conversational question answering (CQA) over knowledge graphs. Specifically, they evaluated the ability of several LLMs—including GPT-3.5-Turbo, LLaMA, Vicuna, and a fine-tuned LoRA variant—to generate executable SPARQL queries from natural language dialogues in the SPICE dataset. The models were tested under both zero-shot and few-shot prompting conditions. The results show that instruction-tuned models like GPT-3.5-Turbo handle logical reasoning well and can produce semantically correct queries, although often not matching the exact gold query. The fine-tuned LoRA-7B model outperformed all others in exact match and accuracy metrics despite its smaller size, demonstrating the effectiveness of task-specific fine-tuning. The authors also conducted a thorough error analysis, identifying common failure modes such as syntax errors, hallucinated entities, and off-prompt outputs. Their findings highlight the potential of LLMs for structured query generation in dialogue systems and the importance of prompt design and domain adaptation.

\paragraph{LLMs Fall Short in Accurate AMR Parsing} 
Recent work by \cite{ettinger2023expert} investigates whether large language models (LLMs) such as GPT-3, ChatGPT, and GPT-4 can serve as expert linguistic annotators in the task of Abstract Meaning Representation (AMR) parsing. While the models are able to reproduce the basic surface format of AMR graphs—demonstrating familiarity with its syntactic structure—they consistently fail to generate semantically accurate parses. Fine-grained manual evaluation across zero-shot, few-shot, and metalinguistic prompting settings reveals that virtually none of the generated parses meet full semantic acceptability. Even with few-shot examples, models struggle with core AMR components such as correct top-node identification, event-argument structure, and modifier alignment. Moreover, similar patterns of semantic error persist in natural language explanations, suggesting that these limitations stem from underlying deficits in semantic analysis capabilities rather than issues with format or prompting alone. These results indicate that LLMs cannot yet be used reliably out-of-the-box for detailed semantic annotation tasks such as AMR parsing.

\paragraph{Parsing with Grammars: Evaluating Language Models on Structured Prediction Tasks}

\citet{bharadwaj2023benchclamp} introduce \textsc{BenchCLAMP}, a benchmark designed to evaluate pretrained language models on structured prediction tasks across nine datasets spanning seven output formalisms, including semantic and syntactic parsing. The authors propose a unified framework that supports both fine-tuning and few-shot prompting while enabling efficient constrained decoding via task-specific context-free grammars. Their findings show that fine-tuned encoder-decoder models like T5-3B can match or exceed state-of-the-art results on tasks like SMCalFlow and TreeDST. Constrained decoding proves especially beneficial in low-resource settings and tasks requiring structural correctness, such as dependency and constituency parsing. The benchmark also highlights that without such constraints, large language models often produce invalid outputs, especially in few-shot scenarios. Overall, BenchCLAMP provides a systematic and extensible way to assess how well language models handle structured output generation, bridging a gap in current LLM benchmarks.

\subsection{Mechanical Turk and the Promise—and Limits—of Crowdsourced Annotation}

The emergence of crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) marked a transformative moment in how the NLP community approached data annotation. Initially seen as a solution to the bottlenecks of expert annotation—especially in scaling to large datasets—MTurk enabled rapid, low-cost collection of labeled data from a distributed pool of non-expert workers. Early work highlighted both the promise and pitfalls of this strategy, and over time, a more nuanced understanding of its limitations has emerged.

\paragraph{The Promise.} Amazon introduced Mechanical Turk in 2005 as a platform for ``artificial artificial intelligence'' \citep{borthwick2005mechanical}, designed to crowdsource tasks that were hard for machines but trivial for humans. This infrastructure was quickly adopted by NLP researchers. \citet{snow2008cheap} conducted one of the first systematic evaluations of MTurk for linguistic annotation, demonstrating that the aggregation of non-expert judgments could rival expert-level annotations across tasks such as word sense disambiguation and textual entailment. Crucially, this study provided early evidence that **annotation quality could be recovered through redundancy and statistical modeling**, thereby opening the door for broader adoption of crowdsourced labeling.

In parallel, \citet{callison2009fast} applied MTurk to the evaluation of machine translation output, showing that inexpensive crowd-based assessments correlated well with expert judgments. These findings established MTurk not only as a tool for dataset construction, but also as a viable component of evaluation pipelines. The Stanford NLI dataset \citep{bowman2015large}, a cornerstone of modern semantic inference research, was built using a hybrid approach: sentence pairs were generated programmatically and then validated by MTurk annotators. This demonstrated how **crowdsourcing could scale** the creation of complex semantic datasets when combined with automation and careful task design.

\paragraph{The Limits.} Despite early optimism, several studies began to identify important caveats. \citet{fort2011amazon} critically examined MTurk's role in NLP, warning of its **ethical blind spots**—especially low pay rates, lack of labor protections, and the cognitive toll of repetitive or emotionally taxing tasks. They also pointed to inconsistencies in annotation quality, especially in subjective or ambiguous tasks, where worker motivation and understanding were difficult to control.

Subsequent work has emphasized that the **assumptions behind crowdsourcing often mask deeper structural problems**. \citet{sabou2014corpus} argued for best-practice guidelines in crowdsourced annotation, highlighting the need for clear instructions, quality control mechanisms, and fair treatment of annotators. \citet{paullada2021data} provided a broader critique, noting that reliance on large-scale crowd annotation has often led to **datasets that are poorly documented, unrepresentative, or ethically problematic**. They argue for a data-centric reevaluation of machine learning pipelines, in which the origin, curation, and social implications of datasets are treated as core scientific concerns.

\paragraph{Reflection.} Taken together, these works illustrate the dual nature of crowdsourced annotation. On one hand, platforms like MTurk have made it possible to scale up dataset construction rapidly and affordably, which has been vital to the development of large neural models. On the other hand, the limitations—both practical and ethical—have become increasingly visible, especially as annotation tasks grow more complex and value-laden. In contemporary NLP, annotation practices are beginning to shift toward more curated, expert-driven, or hybrid systems (e.g., combining LLM-generated suggestions with human verification), as researchers grapple with how to align annotation quality, fairness, and sustainability.
