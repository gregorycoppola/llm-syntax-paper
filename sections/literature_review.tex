% Literature review section content will go here
Recent work has explored the ability of large language models (LLMs) to serve as syntactic annotators, generating linguistic labels such as part-of-speech (POS) tags, dependency trees, and constituency parses. These models operate in zero-shot, few-shot, or fine-tuned settings and exhibit varying levels of success depending on the task, language, and model scale.

However, despite these advances, fundamental questions remain about the linguistic competence of LLMs. For instance, \citet{hu2020systematic} present a large-scale evaluation of syntactic generalization that reveals several striking limitations in current neural language models. Contrary to expectations, perplexity—a standard benchmark for language modeling—was poorly correlated with syntactic generalization ability: models with lower perplexity often performed worse on syntax-targeted tests, undermining the assumption that broad coverage leads to deeper linguistic competence. Standard LSTM architectures, even when trained on tens of millions of tokens, failed to robustly learn basic syntactic phenomena such as subject–verb agreement, and performed at near-chance levels on long-distance dependencies and center-embedding constructions. Transformer-based models like GPT-2 achieved stronger performance, but much of this success was attributed to preprocessing artifacts such as subword tokenization rather than architectural advantages. Perhaps most notably, even the best models consistently failed on syntactic licensing tests, such as those involving negative polarity items—phenomena that humans acquire early and handle reliably. These results suggest that many linguistic generalizations that are trivial for humans remain surprisingly elusive for even the most capable neural language models.

\subsection{POS Tagging and Shallow Syntax}

A key development in standardized syntactic annotation came with \citet{petrov2012universal}'s introduction of a universal part-of-speech tagset, mapping diverse language-specific tag inventories to a shared 12-tag schema to support multilingual and cross-lingual syntactic processing. This standardization has been crucial for evaluating POS tagging performance across languages and models.

\subsection{Dependency Parsing}

Moving from shallow to deep syntax, dependency parsing remains a more challenging task for LLMs. Traditional approaches rely heavily on supervised learning and task-specific architectures. However, \citet{hromei2024udeppllama}'s U-DepPLLaMA reframes dependency parsing as a sequence-to-sequence task, where the model learns to generate bracketed representations of dependency trees directly from raw sentences. Without relying on task-specific architecture, U-DepPLLaMA achieves competitive performance across 50 Universal Dependencies treebanks in 26 languages, demonstrating that autoregressive LLMs can internalize structured syntactic representations through fine-tuning.

The challenge of multilingual dependency parsing has been a longstanding focus of research. \citet{mcdonald2011multi} demonstrated that delexicalized dependency parsers could be effectively transferred across languages, showing that syntactic structure could be learned independently of lexical content. This insight has influenced modern approaches to cross-lingual parsing, including recent work with LLMs.

\paragraph{The CoNLL Format.} The CoNLL shared tasks have played a central role in advancing dependency parsing by establishing standardized formats and evaluation frameworks. The original CoNLL-X shared task \citep{buchholz2006conll} introduced multilingual dependency treebanks in a unified format. This was followed by CoNLL 2007 \citep{nivre2007conll}, which expanded language coverage and improved annotation consistency.

A pivotal development in representation came with \citet{de2006generating}'s Stanford Typed Dependencies, which provided a syntactically and semantically motivated dependency scheme. These efforts influenced the Universal Dependencies (UD) project \citep{nivre2016universal}, which today serves as the primary framework for evaluating dependency parsers in the CoNLL-U format.

\subsection{Constituency Parsing}

Constituency parsing has also seen improvement via LLM prompting. \citet{tian2024chunkprompt} introduced a chunk-then-parse prompting approach that uses chain-of-thought (CoT) reasoning to enhance GPT-4’s parsing performance. This method improves the recovery of deeper syntactic structures in both English and Chinese.

\subsection{Zero-Shot Dependency Parsing}

Recent research has explored whether LLMs like ChatGPT can perform dependency parsing in a zero-shot setting. \citet{lin2023chatgpt} investigated whether ChatGPT could generate dependency trees when prompted directly in CoNLL format. While the model showed some emergent syntactic awareness and produced partially valid parses, performance remained far below that of supervised parsers, with LAS scores of only 28.61 on PTB and 11.93 on CTB5. Outputs also required significant post-processing due to formatting inconsistencies and labeling errors. Other LLMs (e.g., Vicuna, HuggingChat) failed to generate valid output even with one-shot examples.

Thus, while promising, zero-shot dependency parsing with ChatGPT is not yet reliable. However, the observed generalization suggests potential for supporting downstream linguistic analysis.

\subsection{Semantic Parsing and Compositional Semantics}

The relationship between syntactic and semantic parsing has long been a central concern in NLP. Early work by \citet{zelle1996} introduced one of the first data-driven semantic parsers, mapping natural language to Prolog queries via inductive logic programming. This line of research evolved through grammar-based and statistical approaches.

\citet{zettlemoyer2005} proposed a method for learning Combinatory Categorial Grammars (CCGs) from utterance-logical form pairs. This was extended by \citet{zettlemoyer2007online}, who introduced an online learning approach for more scalable parsing. \citet{bos2004} demonstrated wide-coverage semantic representations with CCGs, and \citet{kwiatkowski2010} showed how probabilistic grammars could be induced from logical forms.

An alternative view came from \citet{wong2006learning}, who framed semantic parsing as a machine translation problem using synchronous grammars. \citet{wong2007} incorporated lambda calculus to enhance compositional expressivity.

Later work moved toward weak supervision. \citet{clarke2010} learned from question-answer pairs instead of logical forms, introducing supervision via task outcomes. \citet{goldwasser2011confidence} and \citet{artzi2011} leveraged weak signals and bootstrapping from dialogue data. A major step came with \citet{liang2013learning}'s Dependency-Based Compositional Semantics (DCS), which induced semantic parsers from QA supervision using a dependency-style formalism.

This body of work underscores how structured syntax informs compositional semantics, laying the groundwork for evaluating how LLMs internalize both levels of linguistic structure.

\subsection{Agentic Prompting for Translation Tasks}

Building on work that explores LLMs as linguistic annotators \citep[e.g.,][]{lai2023chatgptpos, blevins2023llmpos}, \citet{jiao2024gradable} examine whether LLMs can behave as adaptive agents in machine translation. Their approach avoids retraining or fine-tuning, instead using structured prompt engineering to steer ChatGPT’s outputs across a range of linguistic complexity.

They introduce the \textbf{T3S taxonomy}, which categorizes prompts into five levels of increasing linguistic scaffolding. These include dimensions such as expression type, translation style, POS information, and few-shot examples—mirroring annotation strategies used in syntactic tasks. Higher levels, like Level 4, include self-revision steps that encourage agentic reasoning.

Experiments on the FLORES-101 Chinese--English dataset show strong correlations between prompt complexity and translation quality, with Level 4 prompts outperforming even GPT-4’s zero-shot baseline. These findings suggest that prompt design can significantly enhance LLM performance on complex tasks like translation, reinforcing the idea that LLMs can act as structured, agentic language processors.

\paragraph{Conclusion.}
Taken together, these studies highlight the expanding capacity of LLMs to perform syntactic and semantic analysis across languages and tasks. From POS tagging to deep compositional semantics, LLMs increasingly demonstrate internalized linguistic structure. Yet consistent challenges in structured prediction—especially for multilingual and zero-shot scenarios—indicate that traditional parsing insights remain essential. This review sets the stage for our own investigation into [insert your focus here], which further explores the interface between LLM prompting, structure induction, and linguistic generalization.

