\subsection{Mechanical Turk and the Promise—and Limits—of Crowdsourced Annotation}

The emergence of crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) marked a transformative moment in how the NLP community approached data annotation. Initially seen as a solution to the bottlenecks of expert annotation—especially in scaling to large datasets—MTurk enabled rapid, low-cost collection of labeled data from a distributed pool of non-expert workers. Early work highlighted both the promise and pitfalls of this strategy, and over time, a more nuanced understanding of its limitations has emerged.

\paragraph{The Promise.} Amazon introduced Mechanical Turk in 2005 as a platform for ``artificial artificial intelligence'' \citep{borthwick2005mechanical}, designed to crowdsource tasks that were hard for machines but trivial for humans. This infrastructure was quickly adopted by NLP researchers. \citet{snow2008cheap} conducted one of the first systematic evaluations of MTurk for linguistic annotation, demonstrating that the aggregation of non-expert judgments could rival expert-level annotations across tasks such as word sense disambiguation and textual entailment. Crucially, this study provided early evidence that annotation quality could be recovered through redundancy and statistical modeling, thereby opening the door for broader adoption of crowdsourced labeling.

In parallel, \citet{callison2009fast} applied MTurk to the evaluation of machine translation output, showing that inexpensive crowd-based assessments correlated well with expert judgments. These findings established MTurk not only as a tool for dataset construction, but also as a viable component of evaluation pipelines. The Stanford NLI dataset \citep{bowman2015large}, a cornerstone of modern semantic inference research, was built using a hybrid approach: sentence pairs were generated programmatically and then validated by MTurk annotators. This demonstrated how crowdsourcing could scale the creation of complex semantic datasets when combined with automation and careful task design.

\paragraph{The Limits.} Despite early optimism, several studies began to identify important caveats. \citet{fort2011amazon} critically examined MTurk's role in NLP, warning of its ethical blind spots—especially low pay rates, lack of labor protections, and the cognitive toll of repetitive or emotionally taxing tasks. They also pointed to inconsistencies in annotation quality, especially in subjective or ambiguous tasks, where worker motivation and understanding were difficult to control.

Subsequent work has emphasized that the assumptions behind crowdsourcing often mask deeper structural problems. \citet{sabou2014corpus} argued for best-practice guidelines in crowdsourced annotation, highlighting the need for clear instructions, quality control mechanisms, and fair treatment of annotators. \citet{paullada2021data} provided a broader critique, noting that reliance on large-scale crowd annotation has often led to datasets that are poorly documented, unrepresentative, or ethically problematic. They argue for a data-centric reevaluation of machine learning pipelines, in which the origin, curation, and social implications of datasets are treated as core scientific concerns.

\paragraph{Reflection.} Taken together, these works illustrate the dual nature of crowdsourced annotation. On one hand, platforms like MTurk have made it possible to scale up dataset construction rapidly and affordably, which has been vital to the development of large neural models. On the other hand, the limitations—both practical and ethical—have become increasingly visible, especially as annotation tasks grow more complex and value-laden. In contemporary NLP, annotation practices are beginning to shift toward more curated, expert-driven, or hybrid systems (e.g., combining LLM-generated suggestions with human verification), as researchers grapple with how to align annotation quality, fairness, and sustainability.

\subsection{Large Language Models as Linguists in the Literature}
Recent experiments demonstrate that large language models (LLMs) can support the enrichment of lexical resources such as FrameNet by identifying new lexical units and generating annotated example sentences. Using zero-shot prompting, models like GPT-4o and Claude 3.5 Sonnet achieved high precision in classifying verbs into appropriate semantic frames and produced natural-sounding, grammatically correct example sentences annotated with core frame elements. Moreover, LLMs showed some ability to distinguish between aspectual verb classes (e.g., activities vs. achievements) based on syntactic diagnostics. However, the results also reveal important limitations. The quality of output depends heavily on prompt design, and models sometimes misinterpret verb senses or generate contextually ambiguous examples. Additionally, linguistic diagnostics (such as co-occurrence with temporal adverbials) do not always function reliably when applied to LLMs, due to the models' tendency to produce acceptable-sounding but semantically inconsistent constructions. These findings suggest that while LLMs are not yet capable of fully replacing expert annotation, they can serve as effective collaborators or second annotators in the process of developing semantic resources \citep{koeva2024}.

Recent work demonstrates that large language models (LLMs), such as GPT-3 (text-davinci-002), can accurately simulate human acceptability judgments of verb argument structure constructions, outperforming previous computational and theoretical models \citep{ambridge2024}. Unlike earlier approaches that required extensive hand-coded input, including pre-specified semantic features and manually curated training data, LLMs produce high correlations with human ratings with no such scaffolding. For example, GPT-3’s ratings of English causative constructions showed strong alignment with adult human judgments (e.g., $r = 0.92$, $\tau = 0.80$). Furthermore, the model generalized appropriately to novel verbs, indicating sensitivity to semantic properties. The authors argue that LLMs should be considered executable theories of language acquisition, with their architectures and training regimes encoding empirically testable hypotheses, in contrast to traditional models which rely on oversimplified, intuitively appealing abstractions.

Recent work demonstrates that large language models (LLMs) are capable of sophisticated metalinguistic reasoning across a range of linguistic tasks. Among the evaluated models, OpenAI's \texttt{o1-preview} stands out for its exceptional performance, particularly in tree-based syntactic analysis and abstract phonological rule induction. On tasks involving recursive syntactic structures and syntactic movement, \texttt{o1-preview} significantly outperforms GPT-3.5, GPT-4, and LLaMA 3.1, achieving near-ceiling accuracy in both syntactic tree construction and the identification of movement traces. Crucially, \texttt{o1-preview} also demonstrates the ability to generalize phonological rules from invented datasets, including cases involving unnatural or rare sound patterns. These findings suggest that with sufficient architectural enhancements, LLMs can acquire genuinely abstract grammatical knowledge, not merely memorize surface-level patterns \citep{begus2025}.

\paragraph{} \citet{blevins2023prompting} explore how large language models can be prompted to output structured linguistic analyses, including syntactic trees and morphological segmentation. The study finds that while models like GPT-3 perform well on surface-level morphological tasks, their ability to generate deep hierarchical structure is more limited and depends heavily on prompt phrasing. This paper is one of the first to systematically evaluate the effectiveness of prompting for eliciting explicit linguistic structures, showing both the promise and limits of treating LLMs as linguists via prompt engineering.

\paragraph{} \citet{behzad2023elqa} introduce ELQA, a curated corpus of metalinguistic questions and answers about English designed to evaluate the metalinguistic awareness of LLMs. The questions range from terminology and definitions to error identification and grammaticality judgments. Their analysis shows that while LLMs like GPT-3.5 can answer basic metalinguistic questions well, performance on more technical or abstract queries is mixed. The resource provides a benchmark for treating LLMs as language analysts rather than just generators.

\paragraph{} \citet{wilcox2018filler} examine whether RNN-based language models can learn and generalize filler--gap dependencies, a hallmark of syntactic competence in humans. Using psycholinguistic-inspired experiments, they show that LMs trained purely on text can exhibit sensitivity to unbounded dependencies such as wh-movement, suggesting an implicit understanding of hierarchical structure. However, their generalization is incomplete, and errors increase with more deeply embedded clauses. This work laid early groundwork for probing syntactic generalizations in LMs.

\paragraph{} \citet{gulordava2018colorless} demonstrate that LSTM-based language models trained on multilingual corpora can capture long-distance syntactic dependencies even in nonsensical ``colorless green'' sentences. The authors show that models trained without semantic grounding can still predict syntactically well-formed continuations, supporting the idea that language models develop abstract grammatical representations. This study is widely cited for arguing that hierarchical syntax can emerge from distributional learning alone.

\paragraph{} \citet{matusevych2022trees} test whether RNNs can learn the recursive structure of noun phrases using an artificial grammar learning paradigm. The study shows that RNNs trained on synthetic languages with hierarchical NP structure can generalize to novel recursive inputs. Importantly, this work uses techniques modeled after human learning experiments, providing a strong analogy between human learners and neural networks. It supports the claim that LMs can act like linguists under the right training and testing conditions.

\paragraph{} \citet{yedetore2023stimulus} ask whether LLMs trained on child-directed speech (a limited and more naturalistic corpus) can generalize hierarchical rules. Using Transformer models, they find that such networks struggle with hierarchical generalization, especially when data is sparse. This contributes to the ``poverty of the stimulus'' debate and suggests that LLMs may require more data than humans to achieve similar generalizations---unless architectural enhancements (like chain-of-thought) are introduced. The paper is especially relevant for evaluating the limits of LLMs as cognitive models of linguists.

\paragraph{Alivanistos et al. (2022)}
\citet{alivanistos2022prompting} present a framework in which prompting is used not just as a task input method, but as a probing mechanism to extract structured knowledge from pretrained language models. Specifically, they show how carefully designed prompts can be used to elicit entity and relation triples, effectively reconstructing knowledge graphs. The model's ability to complete these relations without additional fine-tuning demonstrates that LLMs retain a rich latent knowledge base. This aligns with the notion of treating LLMs as linguists, capable of metalinguistic analysis and structure inference when prompted appropriately.

\paragraph{Li et al. (2022)}
\citet{li2022probing} propose replacing traditional diagnostic probes (small classifiers trained on frozen model representations) with learned prompts to assess what language models know. Their approach, called "probing via prompting," involves training discrete or continuous prompts that elicit a model's latent knowledge in a zero-shot or few-shot setting. They show that prompting can serve the same purpose as post-hoc probes, but with the benefit of being closer to the model's natural inference behavior. This methodology strengthens the view that LLMs can be analyzed in the same way we might evaluate a linguistic informant.

\paragraph{Min et al. (2022)}
\citet{min2022rethinking} explore what aspects of in-context learning are responsible for LLM performance. Through a series of controlled experiments, they find that much of the performance can be attributed not to learning from the examples per se, but from strong prior knowledge in the model, especially about label distributions and common class patterns. Even when demonstrations are perturbed or semantically meaningless, models often perform well. This suggests that LLMs behave less like tabula rasa learners and more like agents with pre-existing linguistic knowledge---which can be elicited through structured prompting.

\paragraph{Press et al. (2022)}
\citet{press2022measuring} tackle the "compositionality gap" in LLMs, which is the mismatch between compositional generalization and model performance. They propose a method called "chain-of-thought with intermediate prompting," where tasks are decomposed into simpler subtasks and solved in sequence, with each output fed as context to the next. Their results show that LLMs can learn more compositional functions when intermediate reasoning steps are prompted explicitly. This approach provides a procedural lens on LLMs' linguistic competence, treating them as capable of assembling complex outputs from smaller linguistic primitives.

\paragraph{Zhou et al. (2022)}
\citet{zhou2022least} introduce the method of "least-to-most prompting," a strategy where LLMs are prompted to solve reasoning problems by building from simple to complex sub-problems. This mimics the pedagogical techniques of human learning and shows improved performance on tasks like multi-hop reasoning, arithmetic, and syntax-sensitive parsing. The method emphasizes the value of step-wise syntactic decomposition, and shows that LLMs can learn and apply rule-based structure when guided incrementally---much like a linguist following a deductive analysis.