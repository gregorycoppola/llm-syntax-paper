\label{sec:contributions}

\subsection{Motivation}
This study is motivated by a desire to make {\em logical information retrieval}.
This is a counter-weight to the situation that we have right now with large language models.
As we will review in Section TODO, large language models have several problems.
This pertains to reasoning.
The reasoning abilities of large language models are very low.
A single llm is limited by well-known reasons as to why it is not Turing Complete (TODO: cite).


\subsection{Findings}
The central contribution of this work is to provide concrete evidence that:
\begin{enumerate}
    \item {\bf large language models} can act effectively as {\bf automatic linguists} -- we show that llm's can {\em annotate} important syntactic data with high inter-annotator agreement with our own guidelines and judgement
\end{enumerate}
We believe that this will greatly change the field of artificial intelligence.

We show that in ``out-of-domain tests,'' that the large language model is more accurate and more robust than the {\em Stanford Dependency Parser}, which represents a reasonable baseline for the accuracy of a traditional structural dependency parser.

