\label{sec:contributions}

\subsection{Motivation}
This study is motivated by a desire to make {\em logical information retrieval}.
This is a counter-weight to the situation that we have right now with large language models.
As we will review in Section TODO, large language models have several problems.
This pertains to reasoning.
The reasoning abilities of large language models are very low.
A single llm is limited by well-known reasons as to why it is not Turing Complete (TODO: cite).

LLM's with Chain-of-Thought, including with RL, the situation is a bit better, in the sense that there is perhaps no definite {\em computational} reason why the ``chain of thought'' process would not work.

However, there are practical problems when it comes to making a ``verifiable'' reward function for a lot of domains.
Also, it is not inherently interpretable.
Also, it is not inherently ``type safe'' like real logic is.

We believe that a fully probabilistic but also fully logical solution to the problem will create the best situation going forward.
This is why we want to get the logical forms for sentences.

But, to get the logical form for all sentences, that would require a {\em semantic} analysis of a surface form sentence.

In other words, we would have to make statistical decisions in order to translate the surface utterance to the ``logical form''.

This is something that has been done before by Liang et al.
And, by Steedman et al.
And, we are kind of following in that vein.

We adopt the view that ``syntax'' is the {\em process} of translating from a {\em surface form utterance} to a {\em logical form}, which is in some close relative of {\em first-order logic}.

Thus, the {\em semanics}, in this terminology, is the first-order logical form.
And, the {\em syntax} is the set of structural changes that we have to make on the way to getting there.

In any event, no matter how one divides the matter semantically, the underlying fact is that, in order to translate from surface level form to logical form, many {\em decisions} must be made.
In the machine learning paradigm, this would mean that many {\em statistical} decisions need to be made.

Historically, this would mean training a parser on labeled data.
But, labeled data is expensive, so we want to show that we can actually get the {\em large language model} to annotate this data.

\subsection{Findings}
The central contribution of this work is to provide concrete evidence that:
\begin{enumerate}
    \item {\bf large language models} can act effectively as {\bf automatic linguists} -- we show that llm's can {\em annotate} important syntactic data with high inter-annotator agreement with our own guidelines and judgement
\end{enumerate}
We believe that this will greatly change the field of artificial intelligence.

We show that in ``out-of-domain tests,'' that the large language model is more accurate and more robust than the {\em Stanford Dependency Parser}, which represents a reasonable baseline for the accuracy of a traditional structural dependency parser.

