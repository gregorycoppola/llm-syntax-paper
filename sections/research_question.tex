\label{sec:contributions}

\subsection{Motivation}
This study is motivated by a desire to make {\em logical information retrieval}.
This is a counter-weight to the situation that we have right now with large language models.
As we will review in Section TODO, large language models have several problems.
This pertains to reasoning.
The reasoning abilities of large language models are very low.
A single llm is limited by well-known reasons as to why it is not Turing Complete (TODO: cite).

LLM's with Chain-of-Thought, including with RL, the situation is a bit better, in the sense that there is perhaps no definite {\em computational} reason why the ``chain of thought'' process would not work.

However, there are practical problems when it comes to making a ``verifiable'' reward function for a lot of domains.
Also, it is not inherently interpretable.
Also, it is not inherently ``type safe'' like real logic is.

We believe that a fully probabilistic but also fully logical solution to the problem will create the best situation going forward.
This is why we want to get the logical forms for sentences.

But, to get the logical form for all sentences, that would require a {\em semantic} analysis of a surface form sentence.

In other words, we would have to make statistical decisions in order to translate the surface utterance to the ``logical form''.

This is something that has been done before by Liang et al.
And, by Steedman et al.
And, we are kind of following in that vein.

We adopt the view that ``syntax'' is the {\em process} of translating from a {\em surface form utterance} to a {\em logical form}, which is in some close relative of {\em first-order logic}.

Thus, the {\em semanics}, in this terminology, is the first-order logical form.
And, the {\em syntax} is the set of structural changes that we have to make on the way to getting there.

In any event, no matter how one divides the matter semantically, the underlying fact is that, in order to translate from surface level form to logical form, many {\em decisions} must be made.
In the machine learning paradigm, this would mean that many {\em statistical} decisions need to be made.

Historically, this would mean training a parser on labeled data.
But, labeled data is expensive, so we want to show that we can actually get the {\em large language model} to annotate this data.

Thus, our desire is to show that a large language model {\em can} actually act as a very cheap {\em linguist}.
If that is the case, then it should be possible to make the decisions necessary in order to get syntactic parses for everything in a document.
And, then it should be possible to get semantic parses (logical forms) for every sentence in a document.

Thus, the entire question of whether or not we can make logical forms using large language models boils down to one question: is it possible for large language models to act as ``syntactic annotation labelers.''

In this document, we will show that the answer is ``yes.''

\subsection{Findings}
The central contribution of this work is to provide concrete evidence that:
\begin{enumerate}
    \item {\bf large language models} can act effectively as {\bf automatic linguists} -- we show that llm's can {\em annotate} important syntactic data with high inter-annotator agreement with our own guidelines and judgement
\end{enumerate}
We believe that this will greatly change the field of artificial intelligence.



\begin{enumerate}
    \item replicate past results that {\bf large language models are {\em not} ``zero shot'' CoNLL dependency parsers}
    \item a {\em refined} analysis that shows that llm's can even not do {\em simpler} versions of the CoNLL one-shot task-- but we go further to break the task down into even simpler versions of ``one shot'' dependency parsing, and offer reasons why this could be the case
    \item novel {\em positive} results that show that, with the right prompts, llm's {\em can} give very reliable syntactic annotations
    \item {\bf better than a dependency parser out-of-domain} -- We show that in ``out-of-domain tests,'' that the large language model is more accurate and more robust than the {\em Stanford Dependency Parser}, which represents a reasonable baseline for the accuracy of a traditional structural dependency parser.
    \item {\bf wide-raning analysis} -- we show for a variety of tasks that we can get good annotations from the large language model: {\em part-of-speech tagging}, {\em prepositional phrase attachment} and {\em main verb identification}
\end{enumerate}