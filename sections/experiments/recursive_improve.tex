\subsection{Iterative Parse Repair with an Agentic Architecture}
\label{subsec:agentic-parse-repair}

In a final experiment, we explored whether ChatGPT could not only identify errors in dependency parses but also iteratively improve them using an agentic architecture. Inspired by interactive reasoning frameworks, we implemented a loop in which the model critiques a parse, proposes modifications, and then applies those modifications until no further changes are suggested.

\paragraph{Method.}
Starting from the Stanford dependency parserâ€™s output, we processed 100 English sentences. Each parse was represented in CoNLL-U format. For each sentence, we ran an iterative loop composed of the following steps:

\begin{enumerate}
    \item \textbf{Critique step:} Prompt ChatGPT to identify any errors or weaknesses in the current parse, focusing on attachment and labeling.
    \item \textbf{Revision step:} In a separate prompt, instruct ChatGPT to apply its suggested corrections to produce an updated CoNLL parse.
    \item \textbf{Termination:} Continue the loop until ChatGPT responds that no further improvements are necessary.
\end{enumerate}

\paragraph{Results.}
This iterative process led to substantial edits across the dataset. Key statistics are shown in Table~\ref{tab:agentic-results}.

\begin{table}[h]
\centering
\caption{Agentic Parse Repair Summary (100 Sentences)}
\label{tab:agentic-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total sentences processed & 100 \\
Average iterations per sentence & 2.1 \\
Average tokens per sentence & 12.5 \\
Total dependency arcs & 1,250 \\
Arcs modified by ChatGPT & 380 (30.4\%) \\
Sentences with at least one change & 72 (72\%) \\
Accuracy of changes (manual evaluation) & 89.0\% \\
\midrule
Labeled attachment score (before) & 76.3\% \\
Labeled attachment score (after) & 84.9\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
This experiment demonstrates that ChatGPT can function not only as a parser or critic, but also as an autonomous *editor* of syntactic structure. Through iterative interaction, the model was able to revise nearly one-third of the arcs in the dataset, with a high proportion of those changes judged as valid corrections. Parsing accuracy improved by 8.6 percentage points over the original Stanford parses, despite no access to gold data during the loop.

The success of this architecture supports a new model of syntax-aware NLP: rather than relying on end-to-end structured outputs, we can engage language models in a revision loop, leveraging their strengths in critique, instruction-following, and local reasoning to incrementally improve structured representations. This opens new directions for interactive, hybrid approaches to linguistic annotation and correction.

