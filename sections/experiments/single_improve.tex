\subsection{ChatGPT as a Parse Critic: Evaluating Attachment Errors}
\label{subsec:chatgpt-critic}

In a final experiment, we explored whether ChatGPT could serve not as a parser, but as a syntactic evaluator—identifying errors in pre-existing dependency parses. This role is distinct from generation: instead of asking the model to produce a parse, we asked it to assess one.

\paragraph{Experimental Setup.}
We again used the same 20 ambiguous PP attachment examples from the previous section. Each sentence was parsed using the Stanford dependency parser. We then identified the key ambiguous word (typically a preposition such as \textit{with}) and posed the following prompt to ChatGPT API:

\begin{quote}
    \textit{Here is a dependency parse of the sentence. Do you see any errors involving the word “X”?}
\end{quote}

The model was required to return a binary judgment (error/no error), as well as an explanation when it claimed an error was present. We refer to this as the \textit{evaluated reason}. The binary judgments were compared to the gold-standard attachment decisions (from our earlier annotations), and the explanations were hand-evaluated for correctness.

\paragraph{Results.}
ChatGPT's binary judgments correctly identified 19 out of 20 attachment decisions (95\% accuracy), outperforming the Stanford parser's baseline of 50\%. Furthermore, in the 19 cases where ChatGPT claimed an error or correctness, it provided explanations that were judged to be accurate in 18 cases (94.7\%).

\begin{table}[h]
\centering
\caption{Attachment Error Detection and Explanation Accuracy}
\label{tab:attachment-critique}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Binary Accuracy (\%)} & \textbf{Explanation Accuracy (\%)} \\
\midrule
ChatGPT API (parse critique) & 95.0 & 94.7 \\
Stanford Parser (attachment accuracy) & 50.0 & -- \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Significance Analysis.}
As with the prior experiment, the difference in binary accuracy is statistically significant. A one-sided binomial test comparing ChatGPT’s 19/20 performance against a 50\% baseline yields a $p$-value of $2.0 \times 10^{-5}$. The 95\% confidence interval on ChatGPT’s binary accuracy is $[76.4\%,\ 99.1\%]$ using the Wilson score method.

\paragraph{Interpretation.}
These results extend our earlier findings in a new direction: ChatGPT not only interprets syntactic ambiguity better than a traditional parser, but can also \textit{evaluate} parser output and explain its decisions in a linguistically coherent way. This opens the door to new workflows where LLMs act as error detectors or post-hoc critics—particularly useful for ambiguous constructions where syntactic structure and semantics are deeply intertwined. In cases where full structural parsing is not reliable, prompting-based error analysis may be a lightweight alternative.

