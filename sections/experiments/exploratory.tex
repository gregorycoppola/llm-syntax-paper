\subsection{Exploratory Analysis}
\label{subsec:exploratory}

\subsubsection{Interpreting Multiple Valid Parses: Evidence of Latent Syntactic Knowledge}

\label{subsec:pp-attachment}

To complement our quantitative evaluation, we conducted a qualitative case study of one particularly challenging syntactic phenomenon: prepositional phrase (PP) attachment. PP attachment has long been a source of ambiguity in syntactic parsing, and LLMs are known to struggle with it, especially in zero-shot settings.

We selected a naturally occurring sentence from Twitter that involved a misinterpretation of a “with”-phrase. The sentence was:

\begin{quote}
    \textit{She saw the man with the telescope.}
\end{quote}

This sentence is classically ambiguous: the prepositional phrase \textit{“with the telescope”} can attach either to the noun \textit{“man”} (implying the man had the telescope) or to the verb \textit{“saw”} (implying she used the telescope). In this specific instance, the intended reading was the latter, but ChatGPT, when asked to provide a dependency parse, attached the PP to the noun.

Despite this syntactic error in parsing, we then followed up with a prompt asking the model:

\begin{quote}
    \textit{Why might someone interpret this sentence the wrong way?}
\end{quote}

ChatGPT responded with an explanation that correctly identified the ambiguity and discussed both possible attachment sites:

\begin{quote}
    \textit{This sentence is ambiguous because the phrase “with the telescope” could attach either to “the man” or to “saw.” If you attach it to “the man,” it implies the man had the telescope. If you attach it to “saw,” it means she used the telescope to see him. Without additional context, both interpretations are valid.}
\end{quote}

This qualitative result suggests that while ChatGPT does not reliably produce the correct parse in structurally ambiguous sentences, it is capable of *describing* the ambiguity and articulating the competing interpretations. This supports the idea that LLMs have access to latent syntactic representations and world knowledge that can be activated in interactive settings, even when they fail to apply that knowledge in structured parsing tasks.

\subsubsection{Interpreting Multiple Valid Parses: Evidence of Latent Syntactic Knowledge}

In a pair of related examples, we explored how ChatGPT handles noun phrase structures involving quantifiers and partitive constructions. Interestingly, the model produced two different syntactic analyses across two cases—one aligning with a traditional parser, and one diverging from it—yet both interpretations agreed with our own linguistic judgment. This serves as qualitative evidence that ChatGPT possesses flexible and context-sensitive syntactic intuitions, even when its output varies.

In the first case, we examined the sentence:

\begin{quote}
    \textit{I really can not even believe some of this content right now.}
\end{quote}

Here, the parser had incorrectly treated \textit{“some”} as the direct object of \textit{“believe”}, rather than \textit{“content”}, which is the head of the partitive noun phrase \textit{“some of this content”}. ChatGPT's parse reproduced this same error. However, when prompted to reflect on the structure, the model acknowledged that \textit{“content”} is the semantic core of the object, and that \textit{“some”} functions as a quantifier or determiner. Despite the initial parsing error, the model demonstrated an awareness of the correct structural interpretation upon further analysis.

In the second case, we presented the simpler sentence:

\begin{quote}
    \textit{I believe all content.}
\end{quote}

Here, both the parser and ChatGPT assigned \textit{“content”} as the direct object and \textit{“all”} as its determiner—an analysis that aligns with canonical syntactic structures and our own judgment.

Taken together, these examples illustrate that ChatGPT is capable of recognizing multiple, contextually valid parses—even when its initial output is imperfect. The fact that the model agreed with the Stanford parser in one case, and disagreed in another, yet aligned with human linguistic reasoning in both, supports the idea that LLMs possess latent syntactic knowledge that is accessible through interaction and contextual prompting.

\subsubsection{Agreement on a Curated Attachment Dataset}

To further evaluate ChatGPT’s syntactic competence, we conducted a collaborative dataset construction exercise focused on prepositional phrase attachment. In this case, the model was asked to generate a set of 20 syntactically ambiguous sentences, each involving a prepositional phrase whose attachment site (e.g., verb or noun) might plausibly vary depending on interpretation.

First, ChatGPT produced the 20 examples, each annotated with the intended attachment (e.g., “attach to verb” or “attach to noun”). We then manually reviewed each sentence, checking whether the proposed attachment was semantically and syntactically plausible. In all cases, we agreed with the model’s judgments, which demonstrated strong contextual awareness and a high degree of alignment with human intuitions.

To complete the loop, we presented the validated dataset back to ChatGPT and asked it to reassess the attachment decisions. The model reaffirmed the original judgments across the board, offering consistent and well-reasoned justifications for each case. This round-trip evaluation—model proposes, human verifies, model reaffirms—offers compelling evidence that ChatGPT is not merely pattern-matching surface strings, but is capable of maintaining coherent and linguistically-informed attachment decisions over multiple turns of interaction.

This experiment also supports the claim that LLMs can be used as collaborative linguistic tools—not just for generation or analysis in isolation, but for *co-construction* and *validation* of structured linguistic phenomena like syntactic ambiguity.

\subsubsection{ChatGPT's Meta-Linguistic Awareness as Evidence of Linguistic Knowledge}

In an exploratory exchange, we asked ChatGPT whether it "understands linguistic structure" and followed up with a request for an overview of the concept. The model responded with a coherent, multi-level explanation of linguistic structure spanning phonology, morphology, syntax, semantics, pragmatics, discourse, and prosody. Each level was illustrated with concrete examples and tied to both theoretical and computational perspectives.

Notably, the model also referenced formal tools such as context-free grammars, dependency trees, and semantic graphs—demonstrating an awareness of how linguistic structure is operationalized in natural language processing systems.

This interaction provides qualitative evidence that ChatGPT possesses a form of meta-linguistic knowledge. It is not merely able to produce syntactically and semantically well-formed sentences, but can also discuss the components and mechanisms that underlie those structures. This capacity supports the claim that LLMs encode structured linguistic representations that can be surfaced through reflective or explanatory prompts.

While such responses do not constitute full-blown theoretical understanding in the human sense, they suggest that ChatGPT's language generation is informed by a robust, if implicit, grasp of linguistic principles.
