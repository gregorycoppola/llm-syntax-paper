\subsection{Exploratory Analysis}
\label{subsec:exploratory}

\subsubsection{Interpreting Multiple Valid Parses: Evidence of Latent Syntactic Knowledge}

\label{subsec:pp-attachment}

To complement our quantitative evaluation, we conducted a qualitative case study of one particularly challenging syntactic phenomenon: prepositional phrase (PP) attachment. PP attachment has long been a source of ambiguity in syntactic parsing, and LLMs are known to struggle with it, especially in zero-shot settings.

We selected a naturally occurring sentence from Twitter that involved a misinterpretation of a “with”-phrase. The sentence was:

\begin{quote}
    \textit{She saw the man with the telescope.}
\end{quote}

This sentence is classically ambiguous: the prepositional phrase \textit{“with the telescope”} can attach either to the noun \textit{“man”} (implying the man had the telescope) or to the verb \textit{“saw”} (implying she used the telescope). In this specific instance, the intended reading was the latter, but ChatGPT, when asked to provide a dependency parse, attached the PP to the noun.

Despite this syntactic error in parsing, we then followed up with a prompt asking the model:

\begin{quote}
    \textit{Why might someone interpret this sentence the wrong way?}
\end{quote}

ChatGPT responded with an explanation that correctly identified the ambiguity and discussed both possible attachment sites:

\begin{quote}
    \textit{This sentence is ambiguous because the phrase “with the telescope” could attach either to “the man” or to “saw.” If you attach it to “the man,” it implies the man had the telescope. If you attach it to “saw,” it means she used the telescope to see him. Without additional context, both interpretations are valid.}
\end{quote}

This qualitative result suggests that while ChatGPT does not reliably produce the correct parse in structurally ambiguous sentences, it is capable of *describing* the ambiguity and articulating the competing interpretations. This supports the idea that LLMs have access to latent syntactic representations and world knowledge that can be activated in interactive settings, even when they fail to apply that knowledge in structured parsing tasks.

\subsubsection{Interpreting Multiple Valid Parses: Evidence of Latent Syntactic Knowledge}

In a pair of related examples, we explored how ChatGPT handles noun phrase structures involving quantifiers and partitive constructions. Interestingly, the model produced two different syntactic analyses across two cases—one aligning with a traditional parser, and one diverging from it—yet both interpretations agreed with our own linguistic judgment. This serves as qualitative evidence that ChatGPT possesses flexible and context-sensitive syntactic intuitions, even when its output varies.

In the first case, we examined the sentence:

\begin{quote}
    \textit{I really can not even believe some of this content right now.}
\end{quote}

Here, the parser had incorrectly treated \textit{“some”} as the direct object of \textit{“believe”}, rather than \textit{“content”}, which is the head of the partitive noun phrase \textit{“some of this content”}. ChatGPT's parse reproduced this same error. However, when prompted to reflect on the structure, the model acknowledged that \textit{“content”} is the semantic core of the object, and that \textit{“some”} functions as a quantifier or determiner. Despite the initial parsing error, the model demonstrated an awareness of the correct structural interpretation upon further analysis.

In the second case, we presented the simpler sentence:

\begin{quote}
    \textit{I believe all content.}
\end{quote}

Here, both the parser and ChatGPT assigned \textit{“content”} as the direct object and \textit{“all”} as its determiner—an analysis that aligns with canonical syntactic structures and our own judgment.

Taken together, these examples illustrate that ChatGPT is capable of recognizing multiple, contextually valid parses—even when its initial output is imperfect. The fact that the model agreed with the Stanford parser in one case, and disagreed in another, yet aligned with human linguistic reasoning in both, supports the idea that LLMs possess latent syntactic knowledge that is accessible through interaction and contextual prompting.
