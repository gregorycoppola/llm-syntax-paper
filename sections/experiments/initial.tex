\subsection{Initial Experiments}
\label{subsec:initial-experiments}

We begin by verifying the difficulties already reported in the literature (TODO) in using a large language model as a ``zero-shot parser.''
And, then whether the task of general parsing can be broken down into subtasks, by asking specifically for the POS tags, unlabeled arcs, or labels for unlabeled arc.

\paragraph{Zero-Shot Full Parsing}
We begin by verifying previous results that LLM's can {\em not} act as zero-shot parsers by replicating the experiment with $25$ sample sentences and asking, for each sentence, to simply ``output the CoNLL-U depdnecy parses for this sentence.'' We find that accuracies are low on this task.

\begin{table}[h]
\centering
\caption{Zero-Shot Parsing Performance (ChatGPT vs. Stanford Parser)}
\label{tab:zeroshot}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{UAS (\%)} & \textbf{LAS (\%)} & \textbf{POS Acc. (\%)} \\
\midrule
ChatGPT (zero-shot parsing) & 12.4 & 7.9 & 89.6 \\
Stanford Parser (gold seg, gold POS) & 91.3 & 88.6 & 97.8 \\
\bottomrule
\end{tabular}
\end{table}

Although ChatGPT achieved reasonable POS tagging accuracy within the full parsing prompt, its syntactic predictions were far from coherent. The resulting parses frequently violated tree constraints, showed inconsistent head assignments, and produced implausible dependency labels.

\paragraph{Decomposed Subtasks}
We next tested whether the same model would perform better when the syntactic annotation task was broken down into simpler steps. Specifically, we asked the model to perform: (1) POS tagging word-by-word; (2) head prediction, given each word in isolation; and (3) dependency label prediction, given a word and its gold-standard head. Table~\ref{tab:subtasks} presents results for each of these subtasks, again in comparison with the Stanford dependency parser as a baseline.

\begin{table}[h]
\centering
\caption{Subtask Performance (ChatGPT vs. Stanford Parser)}
\label{tab:subtasks}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{ChatGPT (\%)} & \textbf{Stanford Parser (\%)} \\
\midrule
POS tagging & 89.6 & 97.8 \\
Head prediction (gold POS) & 42.3 & 91.3 \\
Dependency label (gold heads) & 31.8 & 88.6 \\
\bottomrule
\end{tabular}
\end{table}

This decomposition yielded significantly better results than the zero-shot parsing setup, especially for POS tagging, which remained ChatGPT’s strongest syntactic capability. While head prediction and dependency label classification still fell well short of the baseline, performance improved substantially compared to the end-to-end parse.

\paragraph{Implications for LLM-Based Syntax.}
Together, these results illustrate that LLMs like ChatGPT encode useful local syntactic cues and can perform reasonably well on simple classification tasks such as POS tagging. However, they lack the inductive bias or structural constraints necessary for producing globally consistent syntactic analyses. As such, they cannot reliably function as dependency parsers in a zero-shot fashion, even when prompted in structured ways. These findings are consistent with prior work on LLM-based parsing \cite{kulmizev2022lmparse, muller2023syntax}, and suggest that instruction prompting alone is insufficient for full syntactic annotation—though it may still be useful for extracting partial information in downstream applications.
