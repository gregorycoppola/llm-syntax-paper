\subsection{Evaluating PP Attachment Disambiguation at Scale}
\label{subsec:pp-attachment-bulk}

To go beyond single-example case studies, we designed a controlled experiment to evaluate how well ChatGPT and a traditional syntactic parser handle prepositional phrase (PP) attachment ambiguity in practice. Our focus was on the classic syntactic ambiguity involving constructions like:

\begin{quote}
    \textit{I saw the man with the binoculars.}
\end{quote}

Here, the phrase \textit{“with the binoculars”} can attach either to the noun \textit{“man”} or the verb \textit{“saw”}. Disambiguating such cases requires not only syntactic knowledge but also world knowledge and pragmatic inference.

\paragraph{Dataset Construction.}
We used ChatGPT (via the web interface) to generate 20 naturally phrased English sentences that each contained a PP with an ambiguous attachment site. We then manually reviewed these examples and used ChatGPT's own interpretation (as judged from follow-up clarification questions) as a gold standard. To ensure reliability, we verified each example ourselves and confirmed that ChatGPT's interpretation aligned with our own in all cases (100\% agreement).

\paragraph{Parser Comparison.}
We then parsed all 20 sentences using the Stanford dependency parser and ChatGPT API (with zero-shot, instruction-style prompting). We evaluated the models on whether they assigned the PP to the correct head (noun vs. verb), as defined by our gold standard.

\paragraph{Results.}
The results were striking: the Stanford parser correctly resolved 10 out of 20 PP attachments (50\%), while ChatGPT API got 19 out of 20 correct (95\%). Table~\ref{tab:pp-attachment-results} summarizes these results.

\begin{table}[h]
\centering
\caption{PP Attachment Accuracy (20 Ambiguous Sentences)}
\label{tab:pp-attachment-results}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Correct} & \textbf{Accuracy (\%)} \\
\midrule
ChatGPT API & 19 / 20 & 95.0 \\
Stanford Parser & 10 / 20 & 50.0 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Statistical Significance.}
Despite the small sample size, the performance gap is statistically significant. We conducted a one-sided binomial test to assess whether ChatGPT's performance could be explained by chance under the null hypothesis of 50\% accuracy. The resulting $p$-value was $2.0 \times 10^{-5}$, indicating strong statistical significance ($p < 0.001$). We also computed a 95\% confidence interval on ChatGPT's accuracy using the Wilson score method, yielding $[76.4\%,\ 99.1\%]$.

These results suggest that ChatGPT possesses a robust ability to resolve PP attachment ambiguities in context—far surpassing the performance of a standard syntactic parser, even without any fine-tuning or external knowledge sources. While a larger dataset would further reinforce this conclusion, the current data already provide statistically strong evidence that LLMs can leverage both syntactic and semantic information to make informed attachment decisions in ambiguous settings.

