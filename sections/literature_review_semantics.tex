
\subsection{Foundational Works in the History of Formal Logic}

The development of formal logic has been shaped by a series of landmark works that laid the foundation for modern logic, mathematics, and computer science. Below, we summarize key contributions from the most influential figures in the field.

\begin{itemize}
  \item \textbf{George Boole} introduced an algebraic approach to logic in \textit{An Investigation of the Laws of Thought}, establishing the basis of Boolean algebra and formalizing logical reasoning through mathematical operations~\cite{boole1854laws}.

  \item \textbf{Gottlob Frege} developed the \textit{Begriffsschrift}, a formal language for pure thought modeled after arithmetic. This work is considered the birth of modern predicate logic, introducing quantifiers and variables into logical syntax~\cite{frege1879begriffsschrift}.

  \item \textbf{Giuseppe Peano} compiled the \textit{Formulaire de mathématiques}, which helped formalize mathematical notation and logical inference. Peano's axioms for arithmetic are still influential in formal systems today~\cite{peano1895formulaire}.

  \item \textbf{Alfred North Whitehead and Bertrand Russell} published \textit{Principia Mathematica}, a monumental three-volume work that aimed to derive all of mathematics from logical axioms. It was one of the first major efforts in logicism~\cite{whitehead1910principia}.

  \item \textbf{Kurt Gödel} published his incompleteness theorems in \textit{Über formal unentscheidbare Sätze}, demonstrating inherent limitations in formal systems like Principia Mathematica~\cite{godel1931uber}.

  \item \textbf{Alonzo Church} addressed the \textit{Entscheidungsproblem}, proving its unsolvability and introducing the concept of lambda calculus, foundational for computation theory~\cite{church1936unsolvable}.

  \item \textbf{Alan Turing} independently solved the \textit{Entscheidungsproblem} in his work \textit{On computable numbers}, introducing the concept of the Turing machine~\cite{turing1936computable}.

  \item \textbf{Alfred Tarski} formulated a rigorous semantic theory of truth in formalized languages, later published in \textit{Logic, Semantics, Metamathematics}~\cite{tarski1935concept}.
\end{itemize}

\subsection{Landmark Papers in LLM-Based Reasoning}

Recent years have witnessed a surge in research focused on enhancing the reasoning capabilities of large language models (LLMs). Among the numerous contributions, five papers stand out for their foundational impact on the field:

\paragraph{Chain-of-Thought Prompting (Wei et al., 2022)}
Chain-of-Thought (CoT) prompting introduced a simple yet remarkably effective technique for improving the reasoning capabilities of large language models (LLMs) by encouraging them to produce intermediate steps before arriving at a final answer. Rather than answering a question in a single pass, CoT prompts the model to “think aloud” by decomposing complex tasks into a sequence of logical or arithmetic steps. This mirrors how humans often approach multi-step reasoning tasks and enables LLMs to better handle challenges like mathematical problem solving and commonsense reasoning. The original paper showed that CoT dramatically improves performance on such tasks, particularly in larger models like PaLM-540B. One notable success is that CoT helped unlock the emergent reasoning abilities latent in LLMs at scale. However, the approach does have limitations. It tends to rely on the model being sufficiently large and pre-trained to already contain latent reasoning abilities, and CoT outputs can be fragile—sensitive to prompt design and formatting. Furthermore, CoT alone does not ensure correctness; it improves reasoning fluency but not necessarily factual accuracy.

\paragraph{Program of Thoughts Prompting (Chen et al., 2022)}
Program of Thoughts (PoT) prompting builds on the insights of CoT by introducing a more structured and explicit form of reasoning, inspired by programming languages. Instead of generating free-form reasoning steps in natural language, PoT encourages LLMs to output pseudo-code that separates logical flow from surface text. This makes the model’s reasoning more modular, interpretable, and potentially executable. The authors demonstrated the benefits of PoT in numerical reasoning and symbolic tasks, where expressing solutions as code—e.g., defining variables, loops, and conditionals—can better align with the structure of the problem. One key strength of this method is that it enables closer integration between the language model and external computational engines, offering a hybrid neuro-symbolic pipeline. However, the success of PoT prompting often depends on careful prompt engineering and the model’s familiarity with code-like syntax. Its reliance on pre-defining structured abstractions can also limit generality across diverse domains.

\paragraph{Tree of Thoughts (Yao et al., 2023)}
Tree of Thoughts (ToT) prompting generalizes the idea of linear reasoning in CoT to a structured exploration over multiple reasoning paths organized as a tree. At each node in the reasoning tree, the model proposes possible continuations—“thoughts”—that represent plausible next steps. These are then evaluated using heuristics or value functions to decide which branches to explore further, enabling deliberate search and backtracking. ToT has shown strong empirical results in tasks requiring planning, decision-making, or combinatorial exploration, such as solving puzzles and logic games. The key innovation lies in allowing LLMs to deliberate and compare alternative reasoning strategies rather than being bound to a single, forward-only chain. However, ToT is computationally expensive, requiring the generation and evaluation of many candidate paths. The effectiveness of the method also depends on the design of scoring mechanisms used to evaluate partial thoughts, which can be heuristic or learned—adding complexity to the system.

\paragraph{Self-Consistency Decoding (Wang et al., 2022)}
Self-Consistency is a decoding strategy that complements CoT by addressing the unreliability of individual reasoning paths. Instead of generating a single output, the model is prompted multiple times to produce a diverse set of reasoning traces, each potentially yielding a final answer. These answers are then aggregated—typically via majority voting—to select the most consistent one. This ensemble-like approach significantly boosts accuracy on reasoning tasks by smoothing over the noise and variability of LLM output. Particularly in arithmetic and symbolic domains, self-consistency reduces the impact of hallucinated or brittle reasoning paths. Its strength lies in leveraging the stochastic nature of LLMs as a feature rather than a bug. Nonetheless, the approach incurs significant computational cost, as it requires sampling dozens (sometimes hundreds) of reasoning paths per query. It also assumes that the correct answer is the most frequent, which may not always hold—especially for ambiguous or adversarial inputs.

\paragraph{ReAct (Yao et al., 2022)}
ReAct introduces a powerful framework that integrates reasoning (thought) and interaction (action) in a single prompting loop. In this setup, the language model is not only prompted to think through a problem but also to interact with external tools such as search engines, calculators, or environments like web pages. The key idea is that LLMs can interleave internal reasoning steps with concrete actions, retrieve new information, and continue reasoning in light of that evidence. ReAct has been used to build interactive agents capable of web navigation, open-domain question answering, and tool-assisted problem solving. A major strength of ReAct is its grounding in real-world actions, which mitigates hallucination and supports verifiable intermediate steps. The framework also promotes transparency by logging both reasoning and action sequences. However, ReAct systems are more complex to deploy, requiring external APIs or environments to be accessible, and robust parsing of intermediate outputs. Designing prompts that manage both thought and action transitions coherently also presents practical challenges.

\paragraph{Least-to-Most Prompting (Zhou et al., 2022)}  
Zhou et al.~\cite{zhou2022least} introduced the \emph{least-to-most prompting} technique, which enhances LLM reasoning by decomposing complex questions into simpler sub-questions. This method enables step-by-step reasoning, allowing models like GPT-3 to outperform traditional chain-of-thought prompting on complex tasks. The technique is inspired by cognitive psychology and significantly improves performance on benchmarks such as GSM8K and MultiArith.

\paragraph{Toolformer (Schick et al., 2023)}  
Toolformer~\cite{schick2023toolformer} proposes a self-supervised approach to teach LLMs how to use external tools, such as calculators or web search APIs, without human-annotated demonstrations. The model selects and inserts API calls during training, learning when and how to use tools to improve its task performance. This work bridges the gap between static LLMs and interactive agents capable of tool use, enhancing performance on tasks requiring factual lookup or computation.

\paragraph{Auto-GPT (Richards, 2023)}  
Auto-GPT~\cite{torantulino2023autogpt} is one of the earliest open-source implementations of autonomous language agents powered by GPT-4. It chains LLM calls with self-reflection, memory, and tool use to achieve high-level goals without constant human supervision. While primarily an experimental system, Auto-GPT sparked widespread interest in autonomous agents and highlighted both the promise and limitations of current LLMs when operating over long contexts and evolving plans.

\paragraph{Reflexion (Shinn et al., 2023)}  
Shinn et al.~\cite{shinn2023reflexion} proposed \emph{Reflexion}, a framework in which LLM agents improve task performance through verbal self-reflection. After failing at a task, the agent generates natural language feedback describing the mistake and uses this reflection to guide future attempts. Reflexion combines elements of reinforcement learning and meta-cognition, and has shown to improve the reliability of language agents across multiple interactive tasks.

\paragraph{Step-Level Math Correction with Reinforcement Learning.}
Li et al.~\cite{li2025stepamc} propose \textit{StepAMC}, a novel framework for step-level automatic math correction using reinforcement learning (RL). Unlike traditional approaches that focus only on the final answer, StepAMC provides fine-grained feedback on each individual step in a student's solution. To address the limitations of binary feedback and shallow reasoning, the authors introduce two key components: a space-constrained policy network to improve the stability and precision of step-wise predictions, and a fine-grained reward network that transforms binary human annotations into continuous reward signals. Experiments on PRM-42K and MSD-22K datasets demonstrate that StepAMC outperforms strong baselines such as DPO and PPO, achieving better alignment with human judgment and greater robustness across correct and incorrect step classifications.


\subsection{A Modern Version of the Traditional Semantic Pipeline}
* describe what the traditional semantic piepline is
* this should import the lit review from the liang paper
