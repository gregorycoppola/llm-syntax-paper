The application of large language models to linguistic analysis presents both opportunities and challenges. While these models have demonstrated remarkable capabilities in understanding and generating human language, their potential for formal linguistic analysis remains largely unexplored. This work seeks to bridge this gap by:
\subsection{Landmark Papers in LLM-Based Reasoning}

Recent years have witnessed a surge in research focused on enhancing the reasoning capabilities of large language models (LLMs). Among the numerous contributions, five papers stand out for their foundational impact on the field:

\begin{itemize}
    \item \textbf{Chain-of-Thought Prompting (Wei et al., 2022)} introduced the idea of prompting LLMs to generate intermediate reasoning steps. This simple but powerful technique led to substantial improvements in tasks requiring multi-step reasoning, such as arithmetic and commonsense QA \cite{wei2022chain}.

    \item \textbf{Program of Thoughts Prompting (Chen et al., 2022)} proposed decoupling reasoning from computation by structuring prompts as pseudo-code. This approach showed improved performance on numerical reasoning tasks and inspired more formalized reasoning pipelines \cite{chen2022program}.

    \item \textbf{Tree of Thoughts (Yao et al., 2023)} extended linear reasoning chains to tree-structured exploration. By enabling deliberation over multiple reasoning paths, this method significantly improved performance on decision-making and planning problems \cite{yao2023tree}.

    \item \textbf{Self-Consistency Decoding (Wang et al., 2022)} improved reasoning reliability by sampling multiple reasoning paths and selecting the most frequent answer. This strategy mitigates errors caused by unstable generation and complements chain-of-thought prompting \cite{wang2022self}.

    \item \textbf{ReAct (Yao et al., 2022)} integrated reasoning and acting by combining thought generation with real-time tool use. ReAct agents can interact with environments (e.g., search engines or calculators) while reasoning, enabling more robust and grounded problem-solving \cite{yao2022react}.
\end{itemize}

These works form the conceptual backbone of current advances in LLM-based reasoning, setting the stage for increasingly capable, general-purpose reasoning agents.

\paragraph{Least-to-Most Prompting (Zhou et al., 2022)}  
Zhou et al.~\cite{zhou2022least} introduced the \emph{least-to-most prompting} technique, which enhances LLM reasoning by decomposing complex questions into simpler sub-questions. This method enables step-by-step reasoning, allowing models like GPT-3 to outperform traditional chain-of-thought prompting on complex tasks. The technique is inspired by cognitive psychology and significantly improves performance on benchmarks such as GSM8K and MultiArith.

\paragraph{Toolformer (Schick et al., 2023)}  
Toolformer~\cite{schick2023toolformer} proposes a self-supervised approach to teach LLMs how to use external tools, such as calculators or web search APIs, without human-annotated demonstrations. The model selects and inserts API calls during training, learning when and how to use tools to improve its task performance. This work bridges the gap between static LLMs and interactive agents capable of tool use, enhancing performance on tasks requiring factual lookup or computation.

\paragraph{Auto-GPT (Richards, 2023)}  
Auto-GPT~\cite{torantulino2023autogpt} is one of the earliest open-source implementations of autonomous language agents powered by GPT-4. It chains LLM calls with self-reflection, memory, and tool use to achieve high-level goals without constant human supervision. While primarily an experimental system, Auto-GPT sparked widespread interest in autonomous agents and highlighted both the promise and limitations of current LLMs when operating over long contexts and evolving plans.

\paragraph{Reflexion (Shinn et al., 2023)}  
Shinn et al.~\cite{shinn2023reflexion} proposed \emph{Reflexion}, a framework in which LLM agents improve task performance through verbal self-reflection. After failing at a task, the agent generates natural language feedback describing the mistake and uses this reflection to guide future attempts. Reflexion combines elements of reinforcement learning and meta-cognition, and has shown to improve the reliability of language agents across multiple interactive tasks.

\subsection{Foundational Works in the History of Formal Logic}

The development of formal logic has been shaped by a series of landmark works that laid the foundation for modern logic, mathematics, and computer science. Below, we summarize key contributions from the most influential figures in the field.

\begin{itemize}
  \item \textbf{George Boole} introduced an algebraic approach to logic in \textit{An Investigation of the Laws of Thought}, establishing the basis of Boolean algebra and formalizing logical reasoning through mathematical operations~\cite{boole1854laws}.

  \item \textbf{Gottlob Frege} developed the \textit{Begriffsschrift}, a formal language for pure thought modeled after arithmetic. This work is considered the birth of modern predicate logic, introducing quantifiers and variables into logical syntax~\cite{frege1879begriffsschrift}.

  \item \textbf{Giuseppe Peano} compiled the \textit{Formulaire de mathématiques}, which helped formalize mathematical notation and logical inference. Peano's axioms for arithmetic are still influential in formal systems today~\cite{peano1895formulaire}.

  \item \textbf{Alfred North Whitehead and Bertrand Russell} published \textit{Principia Mathematica}, a monumental three-volume work that aimed to derive all of mathematics from logical axioms. It was one of the first major efforts in logicism~\cite{whitehead1910principia}.

  \item \textbf{Kurt Gödel} published his incompleteness theorems in \textit{Über formal unentscheidbare Sätze}, demonstrating inherent limitations in formal systems like Principia Mathematica~\cite{godel1931uber}.

  \item \textbf{Alonzo Church} addressed the \textit{Entscheidungsproblem}, proving its unsolvability and introducing the concept of lambda calculus, foundational for computation theory~\cite{church1936unsolvable}.

  \item \textbf{Alan Turing} independently solved the \textit{Entscheidungsproblem} in his work \textit{On computable numbers}, introducing the concept of the Turing machine~\cite{turing1936computable}.

  \item \textbf{Alfred Tarski} formulated a rigorous semantic theory of truth in formalized languages, later published in \textit{Logic, Semantics, Metamathematics}~\cite{tarski1935concept}.
\end{itemize}

\subsection{Foundations of Categorial Grammar}

The origins of categorial grammar can be traced back to the early work of Ajdukiewicz, who proposed a formal system for representing syntactic structure using category assignments and functional composition \cite{ajdukiewicz1935syntaktische}. His pioneering ideas introduced the notion that syntactic categories could be combined through well-defined operations, laying the groundwork for later developments in mathematical linguistics.

Building on this, Bar-Hillel extended Ajdukiewicz's notation and explored its application to natural language syntax, introducing a quasi-arithmetical formalism for syntactic description \cite{barhillel1953quasi}. His work played a crucial role in bridging the gap between philosophical logic and formal grammar, and it anticipated several principles of modern computational linguistics.

A major formalization of categorial grammar was provided by Lambek, who introduced a type-logical approach that framed sentence structure within a logical calculus \cite{lambek1958mathematics}. The Lambek calculus remains a cornerstone of categorial grammar theory, offering a powerful and elegant method for capturing syntactic inference through type composition.

In the modern era, Steedman significantly advanced the theory with his work on Combinatory Categorial Grammar (CCG), a highly lexicalized and flexible grammar formalism \cite{steedman1996surface}. Steedman's contributions have been central to the application of categorial principles in computational linguistics and have influenced a range of systems in syntax and semantics.

Together, these works define the intellectual trajectory of categorial grammar, from its philosophical roots to its current role in formal and computational linguistics.
